{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "In this notebook, we will show you:\n",
    "\n",
    "* How to load a distributional model in python (and where to download distributional models from)\n",
    "* How to train your own Word2Vec model (simple version) and a pointer for training your own distributional model with control over hyperparameters and the option to compare unstable vectors (hyperwords code)\n",
    "* How to get insights into the quality and content of the distributional word representations using:\n",
    "    \n",
    "    (a) Simple cosine similarity operations\n",
    "    \n",
    "    (c) Running standard evaluation\n",
    "    \n",
    "    (b) Clustering \n",
    "\n",
    "* How to run standard evaluations\n",
    "\n",
    "In addition, the notebook contains a small exercise for getting started on Dutch data.\n",
    "    \n",
    "\n",
    "**About this notebook:**\n",
    "\n",
    "This notebook is using python 3.6. It is recommeded to run it using Anaconda (which includes most packages used here). \n",
    "\n",
    "Even though you can deal with embeddings using commonly used libraries such as numpy, the Gensim library is a very easy way of getting a first impression: https://radimrehurek.com/gensim/models/keyedvectors.html\n",
    "\n",
    "We recommed installing it via pip from the command line:\n",
    "\n",
    "`pip install gensim`\n",
    "\n",
    "In addition, this notebook uses:\n",
    "\n",
    "* NLTK (Natural language processing toolkit)\n",
    "* Spacy (optional)\n",
    "* Numpy \n",
    "* Scikit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Downloading or creating a distributional semantic model \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.a Links for downloading existing models\n",
    "\n",
    "Follow the links to browse available models. The sources listed below contain English models trained using different algorithms, data with different degrees of preprocessing and varying hyperparameter settings. Some resources also include models in other languages (even Dutch with a bit of luck).\n",
    "\n",
    "### Large and commonly used models (English):\n",
    "\n",
    "* Google word2vec: can be downloaded from here (follow link in instructions): http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/\n",
    "\n",
    "* GloVe (trained on various corpora): https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "* FastText embeddings (Facebook): https://fasttext.cc/docs/en/english-vectors.html\n",
    "\n",
    "* Models with different algorithms, hyperparamters, dimensions and degrees of preprocessing (e.g. dependency parsing windows):  https://vecto.readthedocs.io/en/docs/tutorial/getting_vectors.html\n",
    "\n",
    "\n",
    "### Various models in English & other languages:\n",
    "\n",
    "* word2vec trained on Wikipedia for various languages (including Dutch): https://wikipedia2vec.github.io/wikipedia2vec/pretrained/\n",
    "\n",
    "* Various algorithms and parameters for English and other languages: http://vectors.nlpl.eu/repository/#\n",
    "\n",
    "* Word2vec wikipedia for English and German: https://github.com/idio/wiki2vec\n",
    "\n",
    "* Facebook's fastText (https://fasttext.cc) for languages other than English: https://fasttext.cc/docs/en/crawl-vectors.html \n",
    "\n",
    "\n",
    "Gensim even lets you download models directly via their api. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.b Creating your own model - the quick out-of-the box way "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required installations\n",
    "\n",
    "* requests: used for downloading data - you can download data manually instead\n",
    "* Gensim: Word2vec implementation for python\n",
    "* An NLP package for preprocessing: The examples here use NLTK ([Natural language processing toolkit](http://www.nltk.org/install.html)). Alternatively, you can use [SpaCy](https://spacy.io/usage/models).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 0: Download a corpus**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the package \"requests\" from the commandline using:\n",
    "\n",
    "    pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download a text corpus using python:\n",
    "# You can also do this manually by following this link: http://www.gutenberg.org/cache/epub/730/pg730.txt\n",
    "\n",
    "# project Gutenberg Oliver Twist as a .txt file:\n",
    "import requests\n",
    "url = 'http://www.gutenberg.org/cache/epub/730/pg730.txt'\n",
    "r = requests.get(url)\n",
    "# Access content and decode bytes to utf-8\n",
    "text = r.content.decode('utf-8')\n",
    "\n",
    "# Write the text to a file and store it in our data directory (or do this step manually)\n",
    "with open('../data/oliver_twist.txt', 'w') as outfile:\n",
    "    outfile.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Preprocess the text**\n",
    "    \n",
    " There are several choices you can make in the preprocessing step. In general, you want to remove everything from the data that may introduce artifacts. You can also consider further regularization steps, such as replacing all numeric characters by the same representation or using lower case spelling for the entire corpus. ATTENTION: There is a trade-off between generalization and information (e.g. Lower- and uppercase spelling can be a relevant distinction, consider apple vs Apple).\n",
    " \n",
    "For smaller corpora, you will most likely want to remove punctuation and perhaps include some more regularization. You can even consider lemmatizing the text. If you inspect larger models (e.g. Google word2vec), you will notice that the vocabulary contains punctiuation (and all kinds of other weird symbols). For such a large dataset, the noise introduced by these things can most likely be neglected. \n",
    "\n",
    " Here, we do the following:\n",
    " \n",
    " * remove punctuation\n",
    " * set everything to lower case\n",
    " * cut the text in sentences, so it can be processed by the vanilla, out of the box word2vec implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this', 'is', 'a', 'test', 'text'],\n",
       " ['let', \"'s\", 'see', 'if', 'this', 'works'],\n",
       " ['test']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "import string\n",
    "\n",
    "def remove_punct(tokens):\n",
    "    \n",
    "    # put to lower case \n",
    "    punct = string.punctuation \n",
    "    # Iterate over punctuation marks and replace them by an empty string one by one:\n",
    "    tokens_clean = []\n",
    "    for t in tokens:\n",
    "        if t not in punct:\n",
    "            tokens_clean.append(t)\n",
    "            \n",
    "    return tokens_clean\n",
    "\n",
    "def preprocess(text):\n",
    "    \n",
    "    clean_sentences = []\n",
    "    sentences = sent_tokenize(text.strip())\n",
    "\n",
    "    for s in sentences:\n",
    "        tokens = word_tokenize(s.lower())\n",
    "        tokens_clean = remove_punct(tokens)\n",
    "        clean_sentences.append(tokens_clean)\n",
    "    return clean_sentences\n",
    "    \n",
    "test = \"This is a test text. Let's see if this works! TEST.\"\n",
    "\n",
    "preprocess(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['what', '_could_', 'the', 'boy', 'be', 'crying', 'for'], [\"'i\", 'hope', 'you', 'say', 'your', 'prayers', 'every', 'night', 'said', 'another', 'gentleman', 'in', 'a', 'gruff', 'voice', \"'and\", 'pray', 'for', 'the', 'people', 'who', 'feed', 'you', 'and', 'take', 'care', 'of', 'you', '--', 'like', 'a', 'christian'], [\"'yes\", 'sir', 'stammered', 'the', 'boy'], ['the', 'gentleman', 'who', 'spoke', 'last', 'was', 'unconsciously', 'right'], ['it', 'would', 'have', 'been', 'very', 'like', 'a', 'christian', 'and', 'a', 'marvellously', 'good', 'christian', 'too', 'if', 'oliver', 'had', 'prayed', 'for', 'the', 'people', 'who', 'fed', 'and', 'took', 'care', 'of', '_him_'], ['but', 'he', 'had', \"n't\", 'because', 'nobody', 'had', 'taught', 'him'], [\"'well\"]]\n"
     ]
    }
   ],
   "source": [
    "# apply to your text corpus \n",
    "\n",
    "# load data:\n",
    "\n",
    "with open('../data/oliver_twist.txt') as infile:\n",
    "    text_oliver = infile.read()\n",
    "\n",
    "# clean: \n",
    "text_oliver_clean = preprocess(text_oliver)\n",
    "print(text_oliver_clean[201:208])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Create word2vec model**\n",
    "\n",
    "Gensim allows you to train your own model. Here, we use a toy example, which should run on your local machine. If you'd like to train a larger corpus, you will most likely need to use a server. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "oliver_w2v = Word2Vec(text_oliver_clean, size = 300, window = 4, min_count =2)\n",
    "oliver_w2v.wv.save_word2vec_format('../models/oliver.txt')\n",
    "oliver_w2v.wv.save_word2vec_format('../models/oliver.bin', binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "mr = Word2Vec(movie_reviews.sents())\n",
    "mr.wv.save_word2vec_format('../models/movies.bin', binary = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Accessing word representations of different models\n",
    "\n",
    "\n",
    "Models may be stored in different (and sometimes a little confusing) formats, but they all boil down to these components:\n",
    "\n",
    "* a matrix of word vectors \n",
    "* a vocabulary\n",
    "* a mapping between vectors in the matrix to the words in the vocabulary (often via indices)\n",
    "\n",
    "Even though there is existing software for inspecting and manipulating vectors (e.g. in the Gensim Word2vec toolkit), you can easily write code yourself using numpy (our preferred way of working). This way, you don't have to rely on non-transparent implementations (remember the analogy example...). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.a Accessing models using the Word2vec toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How to load a stored model:\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "#oliver_w2v = KeyedVectors.load_word2vec_format('../models/oliver.txt', binary=False) \n",
    "oliver_w2v = KeyedVectors.load_word2vec_format('../models/oliver.bin', binary=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is represented internally as a...\n",
      "<class 'gensim.models.keyedvectors.Word2VecKeyedVectors'>\n",
      "\n",
      "The model vocabulary is represented internally as a...\n",
      "<class 'dict'>\n",
      "Some words from the model vocabulary:\n",
      "['the', 'and', 'of', 'to', 'a', 'in', 'he', 'his', 'that', 'was', 'it', 'you', 'with', 'i', 'as', 'had', 'said', 'for', \"'s\", 'him']\n",
      "Information stored in the vocabulary for the word \"man\":\n",
      "Vocab(count:6393, index:64)\n",
      "\n",
      "Representation of an individual word vector:\n",
      "<class 'numpy.ndarray'>\n",
      "Number of vector dimensions: 300\n"
     ]
    }
   ],
   "source": [
    "# Explore the word2vec model as a python object:\n",
    "print('The model is represented internally as a...')\n",
    "print(type(oliver_w2v))\n",
    "print()\n",
    "#####\n",
    "vocabulary = oliver_w2v.vocab\n",
    "print('The model vocabulary is represented internally as a...')\n",
    "print(type(vocabulary))\n",
    "print('Some words from the model vocabulary:')\n",
    "print(list(vocabulary.keys())[:20])\n",
    "print('Information stored in the vocabulary for the word \"man\":')\n",
    "print(vocabulary['man'])\n",
    "print()\n",
    "#####\n",
    "# To access the vector of a particular word, you can simply do the following:\n",
    "vec_word = oliver_w2v['day']\n",
    "# This way, you access the vector as a numpy array\n",
    "print('Representation of an individual word vector:')\n",
    "print(type(vec_word))\n",
    "print('Number of vector dimensions:', len(vec_word))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.b Accessing a model without a specific package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Alternatively, you can write your own code for loading your model as a numpy matrix. \n",
    "# I suggest to do this\n",
    "import numpy as np\n",
    "\n",
    "def load_model(path):\n",
    "    \n",
    "    matrix = []\n",
    "    vocab = []\n",
    "    word2index_dict = dict()\n",
    "    \n",
    "    with open(path) as infile:\n",
    "        lines = infile.read().split('\\n')\n",
    "        \n",
    "    for n, line in enumerate(lines[1:]):\n",
    "        line_list = line.split(' ')\n",
    "        word = line_list[0]\n",
    "        vocab.append(word)\n",
    "        vec = [float(v) for v in line_list[1:]]\n",
    "        matrix.append(vec)\n",
    "        word2index_dict[word] = n\n",
    "        \n",
    "    return np.array(matrix), vocab, word2index_dict\n",
    "        \n",
    "matrix, vocab, word2index_dict = load_model('../models/oliver.txt')    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Inspecting word representations from different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to explore what the model represents! Play around with similarity, nearest neighbors and analogies and try to get a feeling for what the vectors can do. Feel free to load existing models and compare what they represent. The code snippets below continue with the Oliver Twist toy example - so don't be disappointed if it returns nonsense. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.a Simple vector operations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Gensim **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to load a stored model:\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "oliver_w2v = KeyedVectors.load_word2vec_format('../models/oliver.txt', binary=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Man and woman should be more similar than man and dog:\n",
      "True!\n",
      "man-woman 0.9945669020488251\n",
      "man-dog 0.9891082727640325\n"
     ]
    }
   ],
   "source": [
    "# similarity\n",
    "\n",
    "cos_man_woman = oliver_w2v.similarity('man', 'woman')\n",
    "cos_man_dog = oliver_w2v.similarity('man', 'dog')\n",
    "\n",
    "print(f'Man and woman should be more similar than man and dog:')\n",
    "if cos_man_woman > cos_man_dog:\n",
    "    print('True!')\n",
    "    print('man-woman', cos_man_woman)\n",
    "    print('man-dog', cos_man_dog)\n",
    "else:\n",
    "    print('False')\n",
    "    print('man-woman', cos_man_woman)\n",
    "    print('man-dog', cos_man_dog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gave 0.9998911619186401\n",
      "herself 0.999885082244873\n",
      "words 0.9998771548271179\n",
      "glance 0.9998705983161926\n",
      "hill 0.9998586773872375\n",
      "gentle 0.9998571872711182\n",
      "during 0.9998525381088257\n",
      "hastily 0.9998466372489929\n",
      "carefully 0.9998195171356201\n",
      "filled 0.999818742275238\n"
     ]
    }
   ],
   "source": [
    "# nearest neighbors \n",
    "\n",
    "# Tip: use the help function if you want to explore the arguments\n",
    "#help(oliver_w2v.most_similar)\n",
    "nearest_neighbors = oliver_w2v.most_similar('dog', topn=10)\n",
    "for w, cos in nearest_neighbors:\n",
    "    print(w, cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gone 0.9985346794128418\n",
      "far 0.9984749555587769\n",
      "where 0.9984269738197327\n",
      "heard 0.9984047412872314\n",
      "death 0.9984027147293091\n",
      "us 0.9983886480331421\n",
      "found 0.9983691573143005\n",
      "truth 0.9983683228492737\n",
      "nobody 0.9983553886413574\n",
      "much 0.9983428716659546\n"
     ]
    }
   ],
   "source": [
    "# Analogy\n",
    "\n",
    "closest_to_predicted_vec = oliver_w2v.most_similar(positive=['woman', 'king'], negative=['man'], topn=10)\n",
    "\n",
    "for word, cosine in closest_to_predicted_vec:\n",
    "    print(word, cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Numpy (generally applicable) **\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity is one of the most important concepts to understand if you want to work with vectors. It is calculated as shown below:\n",
    "\n",
    "![Cosine similarity](../images/cosine.png \"Logo Title Text 1\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import math\n",
    "# Using numpy:\n",
    "\n",
    "def normalize_vector(vec):\n",
    "   \n",
    "    # magnitude of the vector\n",
    "    mag = math.sqrt(sum([pow(value, 2) for value in vec]))\n",
    "\n",
    "    unit_vec = []\n",
    "\n",
    "    for value in vec:\n",
    "        unit_vec.append(value/mag)\n",
    "    unit_vec = np.array(unit_vec)\n",
    "    \n",
    "    \n",
    "def get_cosine(vec1, vec2):\n",
    "\n",
    "    vec1_norm = normalize_vector(vec1)\n",
    "    vec2_norm = normalize_vector(vec2)\n",
    "\n",
    "    cos = np.dot(vec1_norm, vec2_norm)\n",
    "\n",
    "    return cos\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.b  Standard evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluations sets can be found here:\n",
    "\n",
    "Similarity\n",
    "\n",
    "* WordSim 353: http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/\n",
    "* SimLex 999: https://fh295.github.io/simlex.html\n",
    "* MEN: https://staff.fnwi.uva.nl/e.bruni/MEN\n",
    "* Luong rare words: http://www.bigdatalab.ac.cn/benchmark/bm/dd?data=Rare%20Word\n",
    "\n",
    "\n",
    "\n",
    "Analogy \n",
    "\n",
    "* Google test sets (combined): http://download.tensorflow.org/data/questions-words.txt\n",
    "* Google test sets (semantic and morphological)https://bitbucket.org/omerlevy/hyperwords/src/default/testsets/analogy/\n",
    "* BATS: http://vecto.space/projects/BATS/\n",
    "\n",
    "\n",
    "Gensim already contains functions to run some standard evaluations. ATTENTION: If you already have a Gensim version installed, make sure to update it. (Mine was out of date and the evaluations did not run.) \n",
    "\n",
    "**Discussion question: How can we compare the scores of the similarity and relatedness evaluations? How would you test whether the correlation between the model and human judgments of one model is better than the correlation between model and human judgments of another model?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson score (0.14839820093167067, 0.1889434635540723)\n",
      "Spearman Rho score SpearmanrResult(correlation=0.15437272200685423, pvalue=0.17155258868186654)\n",
      "Proportion of out ov vocabulary words 77.33711048158641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "# Gensim has the evaluation methods built in\n",
    "\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "# no access to actual pairs\n",
    "# if you want to read up on the details, run:\n",
    "#help(oliver_w2v.evaluate_word_pairs)\n",
    "pearson, spearman, oov = oliver_w2v.evaluate_word_pairs(datapath('wordsim353.tsv'))\n",
    "\n",
    "print('Pearson score', pearson)\n",
    "print('Spearman Rho score', spearman)\n",
    "print('Proportion of out ov vocabulary words', oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score, output= oliver_w2v.evaluate_word_analogies(datapath('questions-words.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.c Clustering \n",
    "\n",
    "\n",
    "A nice way of inspecting word vectors is testing how they behave in clustering. Scikit learn offers a number of implementations of different [clustering algorithms](https://scikit-learn.org/stable/modules/clustering.html). \n",
    "\n",
    "\n",
    "Note: This is just to get a first impression. I recommed reading up on clustering evaluation for using larger sets without label annotations. Scikit learn is a good start. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fruits = ['apple', 'orange', 'lemon', 'strawberry', 'tomato']\n",
    "vegetables = ['cucumber', 'pepper', 'carrot', 'zucchini', 'egg_plant']\n",
    "animals = ['cat', 'dog', 'chicken', 'shrimp', 'lion', 'hamster', 'jaguar']\n",
    "abstract_concepts = ['feeling', 'idea', 'thought', 'theory', 'anger', 'aggression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat animal\n",
      "dog animal\n",
      "chicken animal\n",
      "shrimp animal\n",
      "lion animal\n",
      "hamster animal\n",
      "jaguar animal\n",
      "apple fruit\n",
      "orange fruit\n",
      "lemon fruit\n",
      "strawberry fruit\n",
      "tomato fruit\n",
      "cucumber vegetable\n",
      "pepper vegetable\n",
      "carrot vegetable\n",
      "zucchini vegetable\n",
      "egg_plant vegetable\n",
      "feeling abstract\n",
      "idea abstract\n",
      "thought abstract\n",
      "theory abstract\n",
      "anger abstract\n",
      "aggression abstract\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def map_words(words, label, word_label_dict):\n",
    "    for word in words:\n",
    "        word_label_dict[word] = label\n",
    "\n",
    "word_label_dict = dict()\n",
    "map_words(animals, 'animal', word_label_dict)\n",
    "map_words(fruits, 'fruit', word_label_dict)\n",
    "map_words(vegetables, 'vegetable', word_label_dict)\n",
    "map_words(abstract_concepts, 'abstract', word_label_dict)\n",
    "\n",
    "for label, words in word_label_dict.items():\n",
    "    print(label, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chicken oov\n",
      "shrimp oov\n",
      "lion oov\n",
      "hamster oov\n",
      "jaguar oov\n",
      "apple oov\n",
      "lemon oov\n",
      "strawberry oov\n",
      "tomato oov\n",
      "cucumber oov\n",
      "pepper oov\n",
      "carrot oov\n",
      "zucchini oov\n",
      "egg_plant oov\n",
      "aggression oov\n"
     ]
    }
   ],
   "source": [
    "def get_all_vectors(word_label_dict, model):\n",
    "    \n",
    "    vecs = []\n",
    "    words_in_vocab = []\n",
    "    \n",
    "    for word in word_label_dict.keys():\n",
    "        if word in model.vocab:\n",
    "            vec = model[word]\n",
    "            vecs.append(vec)\n",
    "            words_in_vocab.append(word)\n",
    "        else:\n",
    "            print(word, 'oov')\n",
    "    \n",
    "    return np.array(vecs), words_in_vocab\n",
    "\n",
    "\n",
    "vecs, words_in_vocab = get_all_vectors(word_label_dict, oliver_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of clusters 3\n"
     ]
    }
   ],
   "source": [
    "# Clustering doc: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans\n",
    "\n",
    "# as many clusters as classes:\n",
    "\n",
    "n_clusters = len(set(word_label_dict[word] for word in words_in_vocab))\n",
    "print('number of clusters', n_clusters)\n",
    "# abstract vs concrete?\n",
    "#n_clusters = 2\n",
    "y_pred = KMeans(n_clusters=n_clusters, init='random').fit_predict(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 ['cat', 'orange', 'theory', 'anger']\n",
      "1 ['dog', 'thought']\n",
      "0 ['feeling', 'idea']\n"
     ]
    }
   ],
   "source": [
    "predicted_clusters = defaultdict(list)\n",
    "for word, pred_label in zip(words_in_vocab, y_pred):\n",
    "    predicted_clusters[pred_label].append(word)\n",
    "    \n",
    "for label, words in predicted_clusters.items():\n",
    "    print(label, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise for Dutch\n",
    "\n",
    "Spacy for Dutch: https://spacy.io/models/nl\n",
    "\n",
    "Spacy quickstart: https://spacy.io/usage\n",
    "\n",
    "1) Find a Dutch corpus\n",
    "python -m spacy download nl_core_news_sm\n",
    "\n",
    "2) Preprocess it using Spacy for Dutch\n",
    "\n",
    "3) Create a model (using Gensim or something else)\n",
    "\n",
    "4) See if you can get an impression of what it captures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('nl_core_news_sm')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
