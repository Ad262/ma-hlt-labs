{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab3.3 Training an emotion classifier using embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "* [Section 1: Quick introduction to embeddings](#section1)\n",
    "* [Section 2: Loading the emotion data](#section2)\n",
    "* [Section 3: Preparing the training and test data](#section3)\n",
    "* [Section 4: Training and applying the model](#section4)\n",
    "* [Section 5: Generating the test report](#section5)\n",
    "* [Section 6: Applying the classifier to your own text](#section6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Quick introduction to embeddings  <a class=\"anchor\" id =\"section1\"></a> \n",
    "\n",
    "Extracting features manually can get us a long way. In addition to lemma and part-of-speech, people have used other information: features of the previous words (on the left) or the next words (on the right), whether the current word starts with a capital, whether it is an abbreviation, etc.\n",
    "\n",
    "A recent alternative way to create a 'semantic' representation of a word is by word embeddings: mapping words (or phrases) from the vocabulary to vectors of real numbers. Conceptually it involves a mathematical embedding from a space with many dimensions per word to a continuous vector space with a much lower dimension. For this reason, they are called dense representations.\n",
    "\n",
    "In linguistics, word embeddings were discussed in the research area of distributional semantics. The idea is to quantify and categorize semantic similarities between linguistic items based on their distributional properties in large samples of language data. The underlying notion is that \"a word is characterized by the company it keeps\" (Firth). Embeddings are however the weights in the hidden layer of a neural network that is trained to predict the contexts rather than representing the context in a vector directly.\n",
    "\n",
    "### Reference:\n",
    "\n",
    "For a nice explanation how word embedddings can improve classical bag-of-word approaches, check out this page:\n",
    "\n",
    "https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will load pre-trained word embeddings called word2vec, created by Google. The embeddings have 300 dimensions.\n",
    "\n",
    "First, download the file from [Kaggle](https://www.kaggle.com/pkugoodspeed/nlpword2vecembeddingspretrained) or from [Google code archive](https://code.google.com/archive/p/word2vec/). Then, create a folder and unpack the word2vec file in that folder.\n",
    "\n",
    "We will load the embedding model with the Gensim package that we used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import gensim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Change to path to the location of your local copy of the GoogleNews embeddings\n",
    "##### It may take a  minute to load the model\n",
    "path_to_model = '/Users/piek/Desktop/ONDERWIJS/data/word-embeddings/classical-models/GoogleNews-vectors-negative300.bin'\n",
    "word_embedding_model = gensim.models.KeyedVectors.load_word2vec_format(path_to_model, binary=True)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you computer cannot handle big models such as the Google news model, you can also download one of the smaller 'Glove' datasets. These are provided as text files and 'gensim' provides a function to convert and load them. Note that these models have less domensions than the Google model and you need to adapt the number of features in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_file = datapath('/Users/piek/Desktop/ONDERWIJS/data/word-embeddings/classical-models/glove.6B.50d.txt')\n",
    "tmp_file = get_tmpfile(\"test_word2vec.txt\")\n",
    "\n",
    "_ = glove2word2vec(glove_file, tmp_file)\n",
    "word_embedding_model = KeyedVectors.load_word2vec_format(tmp_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of downloading the models to disk, 'gensim' also provides a downloader API to load the model from the web when needed. In the next cell, we use this API to download a word embeddding model trained on tweets. Note that these are GloVe embeddings built using Tweets as the name suggests. These vectors are based on 2B tweets, 27B tokens, 1.2M vocab, uncased. The original source can be found here: https://nlp.stanford.edu/projects/glove/. The 25 in the model name refers to the dimensionality of the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader as api\n",
    "# download the model and return as object ready for use\n",
    "word_embedding_model = api.load(\"glove-twitter-25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the model works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9590821]]\n"
     ]
    }
   ],
   "source": [
    "word1='cat'\n",
    "word2='dog'\n",
    "word1_vector=np.array(word_embedding_model[word1]).reshape(1, -1)\n",
    "word2_vector=np.array(word_embedding_model[word2]).reshape(1, -1)\n",
    "print(cosine_similarity(word1_vector, word2_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading the emotion data  <a class=\"anchor\" id =\"section2\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "filepath = './data/MELD/train_sent_emo.csv'\n",
    "df = pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preparing the training and test data  <a class=\"anchor\" id =\"section3\"></a> \n",
    "\n",
    "The following import are needed again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook, we used CountVectorizer to obtain the full vocabulary of the data set and generate vectors for the one-hot-endcoing of each word. In these vectors, each slot represents a word and a value '1' indicates that the word was present in the utterance and a '0' means absence. This results is large and sparse vector representations for each utterance. We have also seen that we can weight the relevance of a word using the 'TF.IDF' function. This still results in large and sparse vectors but weights are more subtle. The down side is sparseness, lack of generalisation and lack robustness. \n",
    "\n",
    "In the following, we are going to represent the utterances by an embedding representation. In fact, we take the word embedding of each token in the utterance and add these together, after which we take the average. All the embeddings have the same number of dimensions in the same order. So if two tokens have a high weight for one dimension then their co-uccurrence in an utterance will enforce that weight. Note that by adding and taking the average, we normalize for the length of the utterance and the order of the tokens is not relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to define two customized function using 'def' to create an embedding representation for each utterance. These functions are taken from: https://www.kaggle.com/varun08/sentiment-analysis-using-word2vec\n",
    "\n",
    "The first function, called 'featureVecMethod', takes the words of the utterance and the embedding model as parameters. The num_features parameter determines the size of the vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_words =[]\n",
    "known_words = []\n",
    "# Function to average all word vectors in a paragraph\n",
    "def featureVecMethod(words, stopwords, model, modelword_index, num_features):\n",
    "    # Pre-initialising empty numpy array for speed\n",
    "    # This create a numpy array with the length of the num_features set to zero values\n",
    "    featureVec = np.zeros(num_features,dtype=\"float32\")\n",
    "    nwords = 0\n",
    "        \n",
    "    for word in  words:\n",
    "        if not word in stop_words: \n",
    "            if word in index2word_set:\n",
    "                nwords = nwords + 1\n",
    "                featureVec = np.add(featureVec,model[word])\n",
    "            else:\n",
    "                word = word.lower()\n",
    "                if word in index2word_set:\n",
    "                    nwords = nwords + 1\n",
    "                    featureVec = np.add(featureVec,model[word])\n",
    "                    #we keep track of the words detected\n",
    "                    known_words.append(word)\n",
    "                else:\n",
    "                    #we keep track of the unknown words to see how well our model fits the data\n",
    "                    unknown_words.append(word)\n",
    "    # Dividing the result by number of words to get average\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function just deals with all the data and creates the list of input vectors. This function calls the previous function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating the average feature vector\n",
    "def getAvgFeatureVecs(texts, stopwords, model, modelword_index, num_features):\n",
    "    counter = 0\n",
    "    textFeatureVecs = np.zeros((len(texts),num_features),dtype=\"float32\")\n",
    "    for text in texts:\n",
    "        # Printing a status message every 1000th text\n",
    "        if counter%200 == 0:\n",
    "            print(\"Review %d of %d\"%(counter,len(texts)))\n",
    "            \n",
    "        textFeatureVecs[counter] = featureVecMethod(text, stopwords, model, modelword_index,num_features)\n",
    "        counter = counter+1\n",
    "    return textFeatureVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now back to our input data. We iterate over the Pandas frame in the same way as before but now we extract for each utterance the embedding representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piek/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 9989\n",
      "Review 200 of 9989\n",
      "Review 400 of 9989\n",
      "Review 600 of 9989\n",
      "Review 800 of 9989\n",
      "Review 1000 of 9989\n",
      "Review 1200 of 9989\n",
      "Review 1400 of 9989\n",
      "Review 1600 of 9989\n",
      "Review 1800 of 9989\n",
      "Review 2000 of 9989\n",
      "Review 2200 of 9989\n",
      "Review 2400 of 9989\n",
      "Review 2600 of 9989\n",
      "Review 2800 of 9989\n",
      "Review 3000 of 9989\n",
      "Review 3200 of 9989\n",
      "Review 3400 of 9989\n",
      "Review 3600 of 9989\n",
      "Review 3800 of 9989\n",
      "Review 4000 of 9989\n",
      "Review 4200 of 9989\n",
      "Review 4400 of 9989\n",
      "Review 4600 of 9989\n",
      "Review 4800 of 9989\n",
      "Review 5000 of 9989\n",
      "Review 5200 of 9989\n",
      "Review 5400 of 9989\n",
      "Review 5600 of 9989\n",
      "Review 5800 of 9989\n",
      "Review 6000 of 9989\n",
      "Review 6200 of 9989\n",
      "Review 6400 of 9989\n",
      "Review 6600 of 9989\n",
      "Review 6800 of 9989\n",
      "Review 7000 of 9989\n",
      "Review 7200 of 9989\n",
      "Review 7400 of 9989\n",
      "Review 7600 of 9989\n",
      "Review 7800 of 9989\n",
      "Review 8000 of 9989\n",
      "Review 8200 of 9989\n",
      "Review 8400 of 9989\n",
      "Review 8600 of 9989\n",
      "Review 8800 of 9989\n",
      "Review 9000 of 9989\n",
      "Review 9200 of 9989\n",
      "Review 9400 of 9989\n",
      "Review 9600 of 9989\n",
      "Review 9800 of 9989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piek/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "# Calculating average feature vector for training set\n",
    "### This is the number of dimensions in the word2vec model used. \n",
    "###The Google news model has 300 dimensions but if you use a Glove model you may have to adapt this accordinlgy\n",
    "\n",
    "#Converting Index2Word which is a list to a set for better speed in the execution.\n",
    "#Allows for quicker lookup if the words exist\n",
    "index2word_set = set(word_embedding_model.wv.index2word)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "num_features = 25\n",
    "training_vectors = []\n",
    "training_labels = []\n",
    "for index, utterance in enumerate(df['Utterance']):\n",
    "    ### Running this for all data requires a lot of memory and takes about an hour.\n",
    "    ### For teaching purposes, it makes sense to limit the data\n",
    "    ### we limit the data to the first 1000 utterances\n",
    "    ##if index==2000:\n",
    "    ##    break\n",
    "    training_vectors.append(nltk.tokenize.word_tokenize(utterance))\n",
    "    training_labels.append(df['Emotion'].iloc[index])\n",
    "\n",
    "trainDataVecs = getAvgFeatureVecs(training_vectors, stop_words, word_embedding_model, index2word_set, num_features)\n",
    "#### Due to the averaging, there could be infinitive values or NaN values. The next numpy function turns these value to \"0\" scores\n",
    "trainDataVecs = np.nan_to_num(trainDataVecs)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the classifier may take a while. If you laptop cannot handle it, reduce the number of training data. Alternatively, you can use a smaller word2vec embeddings model. Here is a website with many ready to use models: http://vectors.nlpl.eu/repository/\n",
    "\n",
    "You can either choose a model with a smaller vocabulary or with less dimensions. Whatever you choose, make sure you can load the model using the 'gensim' package. If you choose a model with less than 300 dimensions (e.g. 100 or 200), you also need to adapt the value for *num_features* accordingly = 300.\n",
    "\n",
    "Let's inspect our training data a bit more. Depending on the break set for loading the training data, you will have a list of vectors with according length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9989"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainDataVecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the first element in the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector length 25\n",
      "[ 0.18926129 -0.05296415 -0.50138575 -0.24667558  0.2502413   0.23562701\n",
      "  0.9052485  -0.24662258  0.03203001  0.24278942 -0.12382173  0.302268\n",
      " -4.0777287   0.30402184  0.17057772  0.3256581   0.20401253 -0.01058714\n",
      " -0.22620572 -0.5342373   0.15801655 -0.00572142 -0.12884729  0.02147143\n",
      " -0.26485285]\n"
     ]
    }
   ],
   "source": [
    "print('Vector length', len(trainDataVecs[0]))\n",
    "print (trainDataVecs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is simply a list with digits, each representing the averaged weight of the tokens or words that made up the utterance. We can checks the length, which should be '300', '100', '50' or '25', etc. depending on the number of dimensions of the word2vec model that you used.\n",
    "\n",
    "There are two major differences with the bag-of-tokens that we used in the previous notebook:\n",
    "\n",
    "1. the vectors are short\n",
    "2. there are no zero's \n",
    "\n",
    "Instead of *large sparse* vectors, we now have *short dense* vectors representing each utterance. Whereas in the previous representation, each slot in the vector corresponds with a token, now each slot is a weight from the hidden layer to learn to predict others words in the context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is true for each utterance, each having a unique set of values for the same hidden layer weights. These weights now represent the meaning of the utterance for a machine, which can use a similarity function such as cosine similairty to measure the degree of equivalence across these representations. When we inspects any other utterance, we see it is represented in a simlar way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "[-9.12399311e-03 -3.35800022e-01 -2.65363783e-01 -9.92293954e-02\n",
      " -1.23498216e-01  8.11171889e-01  1.51711392e+00  7.17526078e-01\n",
      " -5.45693994e-01  2.19683975e-01 -2.85960019e-01  3.46198073e-03\n",
      " -4.36201954e+00  2.35049993e-01  2.65846014e-01 -5.92328012e-01\n",
      "  9.66439992e-02 -4.67678875e-01 -5.74586034e-01 -4.27910030e-01\n",
      "  8.56918376e-03  9.68838036e-02 -7.06506014e-01  7.15071976e-01\n",
      " -2.31559929e-02]\n"
     ]
    }
   ],
   "source": [
    "print(len(trainDataVecs[1000]))\n",
    "print(trainDataVecs[1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the vectors are compatible, we can compare them in the same way as we did before for the word2vec embeddings of *cat* and *dog*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9054803]]\n"
     ]
    }
   ],
   "source": [
    "word1_vector=np.array(trainDataVecs[0]).reshape(1, -1)\n",
    "word2_vector=np.array(trainDataVecs[1000]).reshape(1, -1)\n",
    "print(cosine_similarity(word1_vector, word2_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training, we use the same labels as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral neutral\n"
     ]
    }
   ],
   "source": [
    "print(training_labels[0], training_labels[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have a numeric representation of each text, based on the embeddings of the words. We feed this to a classifier in the same way as we did in the previous notebooks with the Countvectorizer output.\n",
    "\n",
    "Before we can train the classifier, we stil need to convert the labels to numeric values as we did before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we do that, it may be good to check which words are not in the embedding model and therefore do not contribute to the representation of the utterance. In the above function, we kept track of the unknown words. Now we can inspect this list. We use the *Counter* function to get a frequency count of these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of unknown tokens 0.22693210203618197\n",
      "Number of unknown words 959\n",
      "Number of unknown word tokens: 5996\n",
      "Unknown words counts\n",
      "Counter({'i\\x92m': 580, 'don\\x92t': 447, 'it\\x92s': 388, '...': 361, 'that\\x92s': 299, 'you\\x92re': 268, \"y'know\": 192, 'y\\x92know': 188, 'can\\x92t': 159, 'we\\x92re': 114, 'i\\x92ll': 106, 'didn\\x92t': 91, 'he\\x92s': 89, 'i\\x92ve': 86, 'let\\x92s': 66, 'she\\x92s': 66, 'there\\x92s': 66, 'what\\x92s': 59, 'they\\x92re': 49, '..': 49, 'doesn\\x92t': 44, 'i\\x92d': 43, '....': 43, '\\x91cause': 39, 'we\\x92ll': 36, 'wouldn\\x92t': 32, 'and-and': 30, 'you\\x92ve': 29, 'you\\x92ll': 28, 'no-no-no': 25, 'won\\x92t': 25, 'haven\\x92t': 24, '\\x91em': 24, 'wasn\\x92t': 23, 'couldn\\x92t': 23, 'who\\x92s': 23, 'i\\x92m-i\\x92m': 19, 'isn\\x92t': 19, 'shouldn\\x92t': 15, 'here\\x92s': 15, 'you\\x92d': 15, 'doin\\x92': 14, 'hey-hey': 14, 'no-no-no-no': 13, 'we-we': 13, 'it\\x92s-it\\x92s': 13, 'that-that': 13, 'aren\\x92t': 13, 'monica\\x92s': 12, 'morning\\x92s': 12, 'how\\x92s': 11, 'would\\x92ve': 11, '10': 11, 'chandler\\x92s': 11, 'we\\x92ve': 10, 'must\\x92ve': 9, '30': 9, '.....': 9, 'i-i\\x92m': 9, 'yeah-yeah': 9, 'joey\\x92s': 9, 'oh-ho': 9, '\\x91kay': 8, 'everything\\x92s': 8, 'that\\x92s-that\\x92s': 8, '2': 8, '8': 8, 'hey-hey-hey': 7, 'they\\x92ve': 7, 'what\\x92d': 7, 'um-hmm': 7, 'it\\x92ll': 7, 'he\\x92ll': 7, 'nothin\\x92': 6, 'rachel\\x92s': 6, 'goodacre': 6, 'chick-chick': 6, 'he\\x92d': 6, 'he-': 6, '17': 6, 'this-this': 6, 'it\\x92d': 6, '25': 6, 'one\\x92s': 6, 'rosselini': 6, 'guy\\x92s': 6, 'grandmother\\x92s': 6, 'how\\x92d': 5, 'talkin\\x92': 5, 'treeger': 5, 'how-how': 5, '50': 5, '1': 5, '19': 5, 'somebody\\x92s': 5, '40': 5, '7': 5, 'phoebe\\x92s': 5, 'but-but': 5, 'they-they': 5, 'o\\x92clock': 5, 'she\\x92ll': 5, 'it-it': 5, 'the-the': 5, 'heldi': 5, 'um-mm': 4, 'ameri-can': 4, 'not-i\\x92m': 4, 'yuh-huh': 4, '\\x91sup': 4, 'spacecamp': 4, 'just-just': 4, '\\x91cha': 4, '3': 4, 'look-look': 4, '20': 4, '13': 4, 'when-when': 4, 'you-you-you': 4, 'ross\\x92s': 4, '15': 4, 'remoray': 4, 'if-if': 4, 'mary-angela': 4, 'there\\x92ll': 3, 'or-or': 3, \"i'm-i\": 3, 'hadn\\x92t': 3, 'this-': 3, 'they\\x92ll': 3, 'i-': 3, '\\x91bout': 3, 'some-some': 3, 'lookin\\x92': 3, 'baby\\x92s': 3, '4': 3, '6': 3, 'where\\x92s': 3, 'weren\\x92t': 3, 'women\\x92s': 3, 'wait-wait': 3, 'goin\\x92': 3, 'oberman': 3, 'uh-': 3, '\\x91til': 3, '500': 3, 'they\\x92re-they\\x92re': 3, 'should\\x92ve': 3, '700': 3, 'that-that\\x92s': 3, 'she\\x92s-she\\x92s': 3, 'just\\x97i': 3, '27': 3, 'we\\x92ll-we\\x92ll': 3, 'magioni': 3, \"y'see\": 3, '200': 3, 'and-': 3, 'and-and-and': 3, 'scrud': 3, 'siadic': 3, 'thquirt': 3, 'company\\x92s': 2, 'kl-5': 2, 'gr-6': 2, \"'my\": 2, 'ameri-ccan': 2, 'oh-ho-ho': 2, 'did-did': 2, 'truth-day': 2, \"'bout\": 2, 'five-ish': 2, 'you\\x92ll-you\\x92ll': 2, 'phobo': 2, 'phewbedo': 2, 'phaybobo': 2, 'to\\x97you-you': 2, '15s': 2, 'dwha': 2, '\\x91see': 2, 'saturday\\x92': 2, 'pa-haa': 2, 'nose\\x97i': 2, 'it-it\\x92s': 2, 'elizabeth\\x92s': 2, 'eh-wh\\x97excuse': 2, 'lauer\\x92s': 2, 'are-are': 2, 'and-and-and-and-and': 2, 'people\\x92s': 2, 'mashuga': 2, 'adrienne\\x92s': 2, 'character\\x92s': 2, 'number\\x92s': 2, 'bazida': 2, \"y'ever\": 2, \"d'know\": 2, \"y'have\": 2, 'mesozoic': 2, '21st': 2, 'holierthanthou': 2, 'mcnailshisstudents': 2, 'love-i': 2, 'with-with': 2, 'nuts\\x97oh': 2, 'bobby\\x92s': 2, \"'hey\": 2, 'og-ee-op': 2, '8:30': 2, 'joel-burg': 2, 'ye-ye-yeah': 2, 'semi-first': 2, 'batman\\x92s': 2, 'bond\\x92s': 2, 'so\\x97if': 2, 'ross-ross': 2, 'know\\x97well': 2, 'chickeeeen': 2, 'don\\x92t-don\\x92t': 2, 'geller-green': 2, 'not-gay-stuff': 2, 'back-less': 2, 'anyway\\x97umm': 2, 'dubbies': 2, '6:00': 2, 'enhh': 2, 'wait-wait-wait-wait-wait-wait-wait-wait': 2, 'th-th-that': 2, 'run-of-the-mill-slice-it-right-off': 2, 'uh-hmm\\x97wait': 2, 'tibidaybo': 2, 'we\\x92re-we\\x92re': 2, 'filter-tipped': 2, 'really\\x97i': 2, 'what\\x97y\\x92know': 2, 'to\\x97i': 2, 'huh\\x97hmm': 2, 'there-there': 2, 'everybody\\x92s': 2, 'puzzle\\x97beer': 2, 'we-': 2, 'velula': 2, 'oh-ho-kay': 2, 'begins\\x97stop': 2, '\\x97now': 2, 'bernie\\x92s': 2, 'given\\x92': 2, 'ain\\x92t': 2, 'ben-ben': 2, 'giiiiiiift': 2, 'clock\\x92s': 2, 'here\\x97': 2, 'no-n-n-n-no': 2, 'ohhhhh': 2, 'ooooh-oooh-ooooh': 2, 'de-clawing': 2, 'he-he\\x92s': 2, 'ooh\\x97hey': 2, 'but\\x97i': 2, 'shedder': 2, 'spatters': 2, 'hasn\\x92t': 2, '\\x91okay': 2, 'pheebs\\x92': 2, 'underwear\\x97you': 2, 'delvecchio': 2, 'what-wh-what': 2, 'her-': 2, '-leg': 2, 'ju-': 2, 'real-': 2, 'g-go': 2, 're-wired': 2, 'r-r-richard': 2, 'tellin\\x92': 2, 'think-': 2, 'here\\x97but': 2, '29': 2, 'oh-no-no-no': 2, 'song\\x92s': 2, '\\x91look': 2, 'meaning\\x92': 2, \"it's-it\": 2, 'radio\\x92s': 2, 'mood-killer': 2, \"'me\": 2, '1939': 2, '24': 2, 'look-look-look-look-look': 2, 'hear\\x97\\x97mother': 2, 'it-does': 2, 'anyone\\x92s': 2, 'meddled': 2, 'wonerful': 2, 'capades': 2, 'f-hah': 2, 'flennin': 2, \"do\\x97y'know\": 2, 'be\\x97okay': 2, 'ewww': 2, 'half-caf': 2, 'job-': 2, 'artelle': 2, 'blobbies': 2, 'picture\\x97i': 2, 'waxine': 2, 'could\\x92ve': 2, '5': 2, 'forward\\x97stop': 2, 'cheshhh': 2, 'the-': 2, '48': 2, 'i-i-i-i-i': 2, 'whoa-whoa': 2, 'a-gogo': 2, 'c.h.e.e.s.e.': 2, \"i'm-\": 2, '\\x91ya': 2, '\\x91is': 2, 'gettin\\x92': 2, '1,000': 2, '22': 2, '18': 2, 'it-it-it\\x92s': 2, 'in-in': 2, 'mom\\x92s': 2, 'hurely': 2, 'wife\\x92s': 2, 'your-your': 2, 'please-please': 2, '33': 2, 'she-she': 2, 'that\\x92': 2, '35': 2, 'woman\\x92s': 2, 'man\\x92s': 2, '\\x9274': 2, 'mississ-pete': 2, '41': 2, 'fight\\x92s': 2, 'friend\\x92s': 2, '93': 2, '76': 2, 'hm-mmm': 2, 'where-where': 2, 'wh-what\\x92s': 2, 'we\\x92d': 2, 'ow-ow': 2, '8:00': 2, 'zelner': 2, \"'you\": 2, '45': 2, '2,000': 2, 'no-no-no-no-no': 2, 'dum-dum-dum': 2, 'uh-oh-okay': 2, 'they\\x92d': 2, 'as-as': 2, \"you're-you\": 2, 'knuckle-cracking': 2, 'wh-why': 2, 'monana': 2, '23': 2, '7.50': 2, 'jo\\x92s': 2, 'lover\\x92s': 2, '4:00': 2, 'whoa-whoa-whoa': 2, 'just-': 2, 'now-now': 2, '28': 2, '-ow': 2, \"'it\": 2, 'chatracus': 2, 'everyone\\x92s': 2, 'she\\x92d': 2, 'liam\\x92s': 2, 'bapstein-king': 2, 'that-that-that': 2, 'base-alan': 2, 'bing-geller': 2, 'comin\\x92': 2, 'bearnaise': 2, '717': 2, '\\x91easy': 1, 'tab\\x92': 1, 'ow-oh-oh': 1, 'pain-zine': 1, 'deadened': 1, 'ow-ow-ow-ow': 1, 'someone\\x92s': 1, 'squatternut': 1, 'buash': 1, 'i\\x97y\\x92know': 1, 'parents\\x92': 1, 'they-they-they': 1, 'bing-ette': 1, 'leg-chewing': 1, 'broiling': 1, '4-6': 1, '112': 1, '110': 1, '7143457': 1, 'schemp': 1, 'unmarriable': 1, 'dressy-dress': 1, 'franzblau': 1, '\\x91cut': 1, '\\x91cos': 1, 'back\\x97stop': 1, '2030': 1, 'ni-chou': 1, 'chi-ma': 1, 'but\\x97okay': 1, 'just\\x97there\\x92s': 1, 'okay-dokey': 1, '.what\\x92s': 1, 'oh-hey-hey-hey': 1, 'you-you-you-you': 1, 'clamed': 1, '\\x93aww': 1, 'wait-wait-wait': 1, 'she\\x92d-she\\x92d': 1, 'rape\\x97': 1, 'a\\x97ah-ah': 1, 'pl-place': 1, 'this\\x97ah-ha-ha': 1, 'stripper\\x92s': 1, 'just\\x97if': 1, 'havin\\x92': 1, '1200': 1, 'lacys': 1, 'i…it': 1, 'that…': 1, 'suity-man': 1, 'but\\x97come': 1, 'just\\x97y\\x92know': 1, 'hang-hang': 1, 'like-like-like': 1, 'that\\x97it\\x92s': 1, '-she': 1, 'she-': 1, '.......': 1, 'costalano': 1, 'd.d.s': 1, 'robert\\x92s': 1, 'retiling': 1, 'spackel': 1, 'ah-ah-ah': 1, '........': 1, 'issac\\x92s': 1, 'd.j.-ing': 1, 'bloomingdale\\x92s': 1, 'okayyyyy': 1, 'balded': 1, 'key\\x92s': 1, '98': 1, '90/10': 1, 'wh\\x97no': 1, 'case\\x97\\x97yay': 1, 'callin\\x92': 1, '92.3': 1, 'wxrk': 1, 'ross-a-tron': 1, 'oh-wha-ho': 1, '49': 1, 'jill\\x92s': 1, 'deep-deep-deep': 1, 'trench-coat': 1, 'yeah-no': 1, '12': 1, '\\x91laser': 1, 'floyd\\x92': 1, '\\x91the': 1, 'munchies.\\x92': 1, \"'cookie\": 1, 'need\\x97\\x97what': 1, 'yugoslavian': 1, 'i\\x92ll-i\\x92ll': 1, 'not\\x97that\\x92s-that\\x92s': 1, 'after\\x97i': 1, 'lyin\\x92': 1, 'triskaidekaphobia': 1, 'think\\x97oh': 1, 'a-doin': 1, 'ewwuck': 1, 'seashores': 1, 'who-who\\x92s': 1, 'abandoner': 1, 'care\\x97i-i': 1, 'wenus': 1, 'whooooaaaa': 1, 'hygienist\\x92s': 1, '\\x91albino': 1, 'bob\\x92': 1, 'there\\x92s-there\\x92s': 1, 'oooh-hoo': 1, 'ooh-hoo': 1, 'i\\x97he': 1, 'd-cup': 1, '100': 1, '1,500': 1, 'get\\x97no': 1, 'tuttle\\x92s': 1, '37135': 1, 'that\\x97i': 1, 'greely\\x92s': 1, 'shmair': 1, 'say-i': 1, 'don\\x92t\\x97whoa': 1, 'rach-rach-rach': 1, 'headin\\x92': 1, '1982': 1, 'the\\x97that': 1, '\\x91hi': 1, 'kind\\x92ve': 1, 'erwin\\x92s': 1, 'joshua\\x92s': 1, 'style\\x97what': 1, 'cupert-hewitt': 1, 'j-j-just': 1, '5,000': 1, '300': 1, 'six-letter': 1, '16': 1, 'draddle': 1, 'may-pole': 1, '26': 1, 'was\\x97if': 1, 'stop-stop': 1, 'ho-ho-hold': 1, \"just\\x97y'know\\x97stop\": 1, 'wha\\x97what': 1, 'coast\\x92s': 1, 'yo\\x97you': 1, 'else\\x92s': 1, 'nokululu': 1, 'lafite': 1, 'zillionaire': 1, 'pete-chicago': 1, 'who\\x92s-who\\x92s': 1, 'something\\x92s': 1, '\\x93hey': 1, '\\x93nah': 1, 'place.\\x94': 1, 'j437-a': 1, 'winterberry': 1, 'so\\x92s': 1, 'valente\\x92s': 1, 'momma\\x92s': 1, 'ah-': 1, 'whatever\\x97\\x97ca': 1, 'sandra\\x92s': 1, 'very\\x97really': 1, 'julie\\x92s': 1, 'al-you': 1, 'paleontologists': 1, 'clunked': 1, 'giraffe\\x92s': 1, 'pete\\x92s': 1, 'bowmont\\x92s': 1, 'howie\\x92s': 1, 'she\\x97well': 1, 'officer\\x97fireman': 1, 'you\\x97okay': 1, 'doorman-': 1, 'nngghhh': 1, 'rdtor': 1, 'actor\\x92s': 1, 'west-westmont': 1, 'westburg': 1, '80': 1, 'wo-women': 1, 'whoa\\x97hey\\x97wh-wh-what': 1, 'wh-wh-what': 1, 'other\\x92s': 1, \"that's-that\": 1, 'to\\x97whoa': 1, 'semi-private': 1, 'passin\\x92': 1, 'homosexually': 1, 'luchhi': 1, 'thank-thank': 1, 'wait\\x97oh\\x97hey\\x97huh': 1, 'realilized': 1, \"they're-\": 1, 'then-': 1, 'do-': 1, 'wh-which': 1, 'floopy': 1, 'what\\x92s-what\\x92s': 1, 'zelner\\x92s': 1, 'anything\\x97minute': 1, 'chip\\x92s': 1, 'he\\x92s-he\\x92s': 1, 'hey\\x97ooh': 1, 'is\\x97it\\x92s': 1, 'uh-hey': 1, '80-foot': 1, 'a-five': 1, 'crumbies': 1, 'y\\x92think': 1, 'break-up\\x92s': 1, 'ummmmmmmm': 1, 'date\\x97oh': 1, 'dinner\\x92s': 1, 'lot\\x92s': 1, \"it's\\x97not\": 1, 'pinchable': 1, 'biceps\\x97she': 1, 'a-live': 1, 'oooooooooooooohhhhhhhhhhh': 1, 'gobb': 1, 'scary-ass': 1, '\\x91i': 1, 'fishhook': 1, 'it\\x92': 1, '232': 1, 'c\\x92mon': 1, 'got-i\\x92ve': 1, 'if\\x97no': 1, 'and\\x97oh': 1, 'kyle\\x92s': 1, 'ramoray\\x92s': 1, 'we-we\\x92re': 1, 'gellers': 1, 'might\\x92ve': 1, 'right-': 1, 'the\\x97\\x97wowww': 1, 'bunny\\x92s': 1, 'we\\x92re-we\\x92re-we\\x92re': 1, 'big-': 1, 'no-one-': 1, '9': 1, '46': 1, '47': 1, 'bwah-hah-hah': 1, 'mwwwooooo-hah-hah': 1, 'never-never': 1, 'left-': 1, 'du-du-i': 1, 'see\\x97darnit': 1, 'nesele': 1, 'tolouse': 1, 'nestley': 1, 'over-pronouncing': 1, 'ohh-ho-hooohhh': 1, 'paper\\x92s': 1, '\\x91probably\\x92': 1, 'on-on': 1, 'litman-neurolic': 1, 'i\\x97that\\x92s': 1, 'disgustingtons': 1, 'see\\x97i': 1, 'would-would': 1, 'was-i': 1, 'santa\\x92s': 1, 'th-they': 1, '9-1-1': 1, 'let\\x92s-let\\x92s': 1, 'yeah\\x97wait': 1, '94': 1, '350': 1, 'na-uh': 1, 'ju\\x97\\x97hi': 1, 's-it': 1, 's…': 1, 'is…do': 1, '6-year-old': 1, '1a': 1, 'those-those': 1, 'du-ude': 1, 'bla-bla-bla-bla': 1, 'phial': 1, 'startin\\x92': 1, 'roommate\\x92s': 1, 'raquetball': 1, 'uhhhh': 1, 'hel-lo': 1, \"'excuse\": 1, 'blarrglarrghh': 1, 'kiddin\\x92': 1, 'valentine\\x92s': 1, 'it\\x92s-it\\x92s-it\\x92s': 1, '\\x91c\\x92': 1, '\\x91k\\x92': 1, 'kleinman\\x92s': 1, 'testosteroney': 1, 'indeedy-o': 1, 'scooching': 1, 'willick': 1, '......': 1, 'hm-mm': 1, \"'cha\": 1, 'now\\x97go': 1, 'embryossss': 1, '75': 1, '16,000': 1, 'that\\x97okay': 1, 'johnos': 1, \"we'll-we\": 1, 'are-': 1, 'helloooo': 1, 'm-o-n-a-y': 1, 'zygomatic': 1, 'craniotomy': 1, 'ad-libbing': 1, '\\x91does': 1, 'yes-yes-yes': 1, 'bactine': 1, \"'poor\": 1, 'that\\x92ll': 1, 'shhh': 1, 'runnin\\x92': 1, '555-9323': 1, 'subletting': 1, 'world\\x92s': 1, 'trrrribbiani': 1, 'audition\\x92s': 1, 'that\\x92d': 1, '437': 1, 'are-are-are': 1, 'oh-oi': 1, 'oh-oi-ho': 1, 'oh-whoa-hey': 1, 'gee-e-e-eez': 1, 'not-not': 1, 'agamemnon': 1, 'yeah\\x97no': 1, 'girlfriend\\x97which': 1, 'took-off': 1, 'quick-quick': 1, 'dad\\x92s': 1, 'is-is': 1, \"'co-dependent\": 1, \"'self-destructive\": 1, \"he's-\": 1, 'thought-': 1, 'do-i': 1, 'what-what-what-what': 1, 'it-it\\x97the': 1, 'you\\x97i': 1, 'chan-chan': 1, \"i've-i\": 1, '56': 1, 'sarah\\x92s': 1, 'ohhhh': 1, '321': 1, '278': 1, '-hey': 1, 'woo-hoo-hoo': 1, 'not\\x97no-no-no': 1, 'jus-': 1, 'go.you': 1, 'buddy-boy': 1, 'lee-lo': 1, 'monnnnn': 1, 'beef-tips': 1, 'ramoray': 1, '1989': 1, 'redecorate\\x97and': 1, 'knock-offs': 1, 'here\\x92s-here\\x92s': 1, '60': 1, 'the\\x97okay': 1, '16-hour': 1, 'shoppin\\x92': 1, 'i\\x92m-i\\x92m-i\\x92m': 1, 'paul\\x92s': 1, 'italy—please': 1, 'i…': 1, \"'memo-\": 1, 'up\\x97i': 1, 'unclogging': 1, 'didn\\x92t\\x97i': 1, 'whoa-whoops': 1, 'parent\\x92s': 1, 'just\\x97it\\x92s': 1, 'whoo-ho-hoo': 1, 'whoo-ho-ho': 1, 'no\\x97\\x97one': 1, 'gar\\x92': 1, 'mm-hm': 1, 'like-': 1, 'grandma\\x92s': 1, 'sleeperson': 1, 'brownshirt': 1, 'mcpretty': 1, \"'were\": 1, \"'pretty\": 1, 'orrr': 1, 'fabutec': 1, 'info-mercial': 1, 'never-ever': 1, \"'scuse\": 1, 'varrrrrroom': 1, 'varrrrrrrrrrom': 1, 'varrrrrrrrroom': 1, 'bed\\x92s': 1, 'burring': 1, 'pictures\\x97are': 1, '\\x91where': 1, 'win\\x97ross': 1, '3\\x97go': 1, '2:17': 1, 'you\\x92re\\x97you': 1, 'empire-waisted': 1, 'see-i': 1, 'these-these': 1, '\\x91i\\x92': 1, 'y\\x97': 1, 'ohhhhhhhh': 1, 'hm-hmm': 1, 'i-i-i-i\\x97i': 1, 'good\\x97ohh': 1, 'blurtin\\x92': 1, 'oh-okay': 1, 'off-stage': 1, 'err-err': 1, 'from-': 1, 'hookin\\x92': 1, \"'maybe\": 1, \"'ohh\": 1, \"'yes\": 1, \"'wh-whoa\": 1, \"'hhiii\": 1, \"'man\": 1, 'don\\x92t\\x97maybe': 1, 'just\\x97it': 1, 'did-did-did': 1, 'why\\x92s': 1, 'upset\\x97for': 1, 'uh-hmm': 1, 'again\\x97nooooooo': 1, 'uh\\x97how': 1, 'reconfiguration': 1, 'but\\x97y\\x92know': 1, 'girl\\x92s': 1, 'oh-no': 1, 'moment\\x92s': 1, 'janice\\x92s': 1, '\\x91dear': 1, 'hubba-bubba': 1, 'birthday\\x92': 1, '\\x91mattress': 1, 'king\\x92': 1, 'maaaaadd': 1, 'plan\\x92s': 1, \"'euphoria\": 1, '79': 1, \"'mistress\": 1, 'n.y.p.d': 1, 'come\\x97i': 1, 'believe\\x97i': 1, 'nature\\x92s': 1, 'right-right': 1, 'right\\x97hey': 1, 'bapstein': 1, 'sting\\x92s': 1, 'ben\\x92s': 1, 'base-': 1, 'hassidic': 1, 'alan-ness': 1, 'downstage': 1, 'spackle': 1, 'about-about': 1, 'mean-i\\x92m': 1, 'you\\x92re-you\\x92re': 1, 'mean\\x97no': 1, 'you\\x97you': 1, 'jalepino': 1, '3:30': 1, 'markson': 1, 'fact\\x97yes': 1, 'she-she-she': 1, 'you\\x97yeah': 1, 'schmoon': 1, 'ooh-ooh': 1, 'tedlock': 1, 'kostelick': 1, '\\x97eh': 1, 'devonian': 1, 'groom\\x97no': 1, 'guys\\x92': 1, 'i\\x97rachel': 1, 'ohh\\x97do': 1, 'hydrosaurids': 1, 'hydrosaurs': 1, 'loreo': 1, 'uhh…': 1, 'well…did': 1, 'll-i': 1, 'just…i': 1, 'beaudalire': 1, 'these\\x92ll': 1, 'yentel': 1, 'i-i-i\\x92ve': 1, 'woo-woo': 1, 'alessandro\\x92s': 1, 'shirvel': 1, 'argentinaaaa': 1, 'spleeeen': 1, 'medeio': 1, 'trorro': 1, 'womba': 1, 'be-because': 1, 'oh-oh-oh-oh': 1, 'rattraps': 1, 'possom': 1, 'wisecracking': 1, '5:30': 1, 'you-what': 1, 'whoa-hey-oh': 1, 'hey-hey-hey-ho-ho': 1, 'you-how': 1, 'plinky-plunky': 1, 'or\\x97oh': 1, 'pipe-fitting': 1, 'out-of-work': 1, 'pre-cooking': 1, 'i-i-i\\x92m': 1, 'sister\\x92s': 1, 'teeny-tiny': 1, '\\x91or\\x92': 1, 'please\\x97': 1, 'sullies': 1, '87': 1, 'bleaker': 1, 'lisettie': 1, 'that\\x97this': 1, '11-b': 1, 'u.s.s': 1, 'half-charred': 1, 'picture\\x97wow': 1, \"'pl\": 1, 'g.stephanopoulos': 1, 'snuffalopagus': 1, 'allll': 1, 'no-o-o': 1, 'boredddd': 1, 'pfieffer': 1, 'askin\\x92': 1, 'don\\x92t-you': 1, 'gerston': 1, 'i\\x97it': 1, 'i\\x92ll-i\\x92ll-i\\x92ll-i\\x92ll': 1, 'shakin\\x92': 1, \"o'mally\": 1, 'cowlicky': 1, 'sweepin\\x92': 1, 'but\\x97bye-bye': 1, 'janine\\x92s': 1, 'whippin\\x92': 1, '\\x93oh': 1, 'here\\x97do': 1, 'i-i-i-i\\x92m': 1, 'anybody\\x92s': 1, 'spindler': 1, 'sulkov': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "unknown_words_count = Counter(unknown_words)\n",
    "print('Proportion of unknown tokens', len(unknown_words)/(len(unknown_words)+len(known_words)))\n",
    "print('Number of unknown words',len(unknown_words_count))\n",
    "print('Number of unknown word tokens:', len(unknown_words))\n",
    "print('Unknown words counts')\n",
    "print(unknown_words_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also kept track of the *known* words, so lets check these as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of known words 1398\n",
      "Number of known words tokens: 20426\n",
      "Unknown words counts\n",
      "Counter({'i': 4141, 'oh': 1217, 'you': 637, 'yeah': 610, 'okay': 603, 'no': 558, 'what': 528, 'hey': 523, 'well': 507, 'so': 327, 'and': 288, 'ross': 251, 'god': 237, 'all': 236, 'joey': 216, 'it': 205, 'that': 191, 'hi': 183, 'but': 182, 'we': 169, 'chandler': 153, 'yes': 142, 'rachel': 137, 'look': 136, 'monica': 135, 'this': 134, 'phoebe': 134, 'why': 132, 'the': 126, 'how': 125, 'he': 117, 'uh': 103, 'do': 100, 'come': 100, 'ok': 98, 'now': 97, 'thank': 97, 'really': 95, 'i-i': 93, 'umm': 91, 'pheebs': 87, 'wow': 87, 'she': 77, 'my': 75, 'are': 74, 'just': 64, 'rach': 64, 'hello': 64, 'thanks': 58, 'ohh': 58, 'here': 55, 'can': 54, 'they': 54, 'good': 53, 'is': 53, 'listen': 51, 'did': 51, 'who': 47, 'there': 47, 'let': 46, 'great': 45, 'wait': 45, 'ooh': 44, 'if': 42, 'not': 42, 'because': 41, 'when': 41, 'right': 40, 'see': 39, 'um': 39, 'go': 39, 'maybe': 39, 'a': 39, 'whoa': 37, 'where': 36, 'does': 35, 'sorry': 34, 'huh': 33, 'mr.': 33, 'uh-huh': 33, 'no-no': 33, 'me': 31, 'please': 31, 'ah': 30, 'joe': 30, 'nothing': 30, 'get': 30, 'ben': 30, 'janice': 30, 'geller': 29, 'ow': 29, 'dr.': 27, 'sure': 27, 'fine': 26, 'bob': 25, 'bing': 25, 'of': 24, 'would': 23, 'honey': 23, 'like': 23, 'ahh': 22, 'yep': 21, 'then': 21, 'nice': 21, 'thanksgiving': 20, 'man': 20, 'actually': 20, 'in': 20, 'kathy': 20, 'alright': 20, 'excuse': 19, 'for': 19, 'carol': 19, 'stop': 19, 'which': 18, 'uhh': 18, 'or': 17, 'bye': 17, 'david': 17, 'remember': 17, 'aw': 17, 'i-i-i': 17, 'ugh': 17, 'your': 17, 'anyway': 16, 'one': 16, 'ohhh': 16, 'big': 15, 'very': 15, 'mrs.': 15, 'mon': 14, 'dad': 14, 'jill': 14, 'aww': 14, 'joshua': 14, 'check': 14, 'hmm': 14, 'susan': 14, 'dude': 13, 'richard': 13, 'mom': 13, 'new': 13, 'give': 13, 'casey': 13, 'mark': 12, 'nobody': 12, 'green': 12, 'say': 12, 'oh-oh': 12, 'boutros': 12, 'absolutely': 11, 'last': 11, 'uh-oh': 11, 'ew': 11, 'guys': 11, 'will': 11, 'happy': 11, 'by': 11, 'what-what': 11, 'with': 11, 'emily': 11, 'barry': 11, 'could': 11, 'tell': 11, 'these': 11, 'got': 10, 'ok.': 10, 'damn': 10, 'tribbiani': 10, 'was': 10, 'ma': 10, 'seriously': 10, 'take': 10, 'to': 10, 'have': 10, 'bonnie': 10, 'porsche': 10, 'chip': 10, 'everybody': 9, 'london': 9, 'bye-bye': 9, 'mike': 9, 'tv': 9, 'her': 9, 'although': 9, 'some': 9, 'europe': 9, 'exactly': 9, 'tag': 9, 'nope': 9, 'eh': 9, 'nah': 9, 'drake': 9, 'italian': 8, 'james': 8, 'something': 8, 'stevens': 8, 'first': 8, 'professor': 8, 'jason': 8, 'christmas': 8, 'vegas': 8, 'buffay': 8, 'mona': 8, 'nooo': 8, 'were': 8, 'roger': 8, 'jane': 8, 'rogers': 8, 'hold': 8, 'am': 8, 'bijan': 8, 'monday': 7, 'yay': 7, 'daddy': 7, 'stupid': 7, 'as': 7, 'you-you': 7, 'post': 7, 'julie': 7, 'kip': 7, 'isabella': 7, 'york': 7, 'noo': 7, 'larry': 7, 'issac': 7, 'tulsa': 7, 'done': 6, 'be': 6, 'central': 6, 'even': 6, 'paul': 6, 'ace': 6, 'jack': 6, 'dana': 6, 'morning': 6, 'uncle': 6, 'pretty': 6, 'yemen': 6, 'about': 6, 'looks': 6, 'grandma': 6, 'ms.': 6, 'british': 6, 'john': 6, 'danny': 6, 'our': 6, 'park': 6, 'elizabeth': 6, 'french': 6, 'sergei': 6, 'gali': 6, 'jake': 6, 'father': 6, 'shh': 6, 'shoot': 6, 'either': 6, 'five': 6, 'open': 6, 'frank': 6, 'alan': 6, 'out': 5, 'raymond': 5, 'from': 5, 'ah-ha': 5, 'wh': 5, 'bobby': 5, 'way': 5, 'toby': 5, 'oo': 5, 'santa': 5, 'excellent': 5, 'saturday': 5, 'barcelona': 5, 'congratulations': 5, 'chi': 5, 'dear': 5, 'city': 5, 'pete': 5, 'aunt': 5, 'shut': 5, 'wan': 5, 'always': 5, 'friday': 5, 'smelly': 5, \"c'mon\": 5, 'jamie': 5, 'those': 5, 'san': 5, 'ready': 5, 'anybody': 5, 'chloe': 5, 'relax': 5, 'totally': 5, 'on': 5, 'move': 5, 'little': 5, 'gunther': 5, 'so-so': 5, 'forget': 4, 'i-i-i-i': 4, 'estelle': 4, 'oy': 4, 'pizza': 4, 'calm': 4, 'gina': 4, 'kyle': 4, 'later': 4, 'call': 4, 'since': 4, 'king': 4, 'especially': 4, 'any': 4, 'krista': 4, 'secret': 4, 'atm': 4, 'tomorrow': 4, 'phil': 4, 'guess': 4, 'vince': 4, 'baby': 4, 'girl': 4, 'cassie': 4, 'france': 4, 'western': 4, 'ha-ha': 4, 'cups': 4, 'steady': 4, 'correct': 4, 'julio': 4, 'ho-ho': 4, 'sunshine': 4, 'times': 4, 'sounds': 4, 'ten': 4, 'angela': 4, 'heckles': 4, 'burn': 4, 'left': 4, 'everything': 4, 'both': 4, 'phoebs': 4, 'ole': 4, 'kinda': 4, 'jordie': 4, 'knicks': 4, 'diego': 4, 'ha': 4, 'robert': 4, 'still': 4, 'damnit': 4, 'apparently': 4, 'wha': 4, 'aruba': 4, 'farber': 4, 'welcome': 4, 'cause': 4, 'mine': 4, 'may': 4, 'jim': 4, 'poughkeepsie': 4, 'stay': 4, 'fun': 4, 'whew': 4, 'night': 4, 'paolo': 4, 'george': 4, 'good-bye': 4, 'chuck': 4, 'people': 4, 'noooo': 4, 'had': 4, 'dum': 4, 'hope': 4, 'ursula': 4, 'speech': 4, 'montreal': 4, 'world': 3, 'indian': 3, 'met': 3, 'water': 3, 'breathe': 3, 'dina': 3, 'unless': 3, 'want': 3, 'uma': 3, 'thurman': 3, 'vic': 3, 'such': 3, 'ronni': 3, 'going': 3, 'made': 3, 'internet': 3, 'dammit': 3, 'also': 3, 'sophie': 3, 'watch': 3, 'johnson': 3, 'india': 3, 'atlantic': 3, 'ooohh': 3, 'problem': 3, 'somebody': 3, 'ukrainian': 3, 'band-aid': 3, 'loved': 3, 'singing': 3, 'ladies': 3, 'hmmm': 3, 'goodbye': 3, 'glad': 3, 'use': 3, 'sing': 3, 'saint': 3, 'days': 3, 'oh-oh-oh': 3, 'pick': 3, 'freeze': 3, 'florida': 3, 'krog': 3, 'knock': 3, 'interesting': 3, 'cut': 3, 'only': 3, 'boy': 3, 'basically': 3, 'marcel': 3, 'op': 3, 'oww': 3, 'mac': 3, 'gim': 3, 'mindy': 3, 'chelsea': 3, 'gee': 3, 'ehh': 3, 'doctor': 3, 'thread': 3, 'america': 3, 'sperm': 3, 'brown': 3, 'better': 3, 'lily': 3, 'ed': 3, 'begley': 3, 'jr.': 3, 'ca': 3, 'bitch': 3, 'more': 3, 'wells': 3, 'adam': 3, 'eww': 3, 'plus': 3, 'almost': 3, 'sit': 3, 'four': 3, 'sidney': 3, 'route': 3, 'definitely': 3, 'island': 3, 'matthews': 3, 'naked': 3, 'guy': 3, 'every': 3, 'never': 3, 'heads': 3, 'awww': 3, 'kind': 3, 'bamboozled': 3, 'kristen': 3, 'kristin': 3, 'mm': 3, 'mcdowell': 3, 'amy': 3, 'geez': 3, 'paris': 3, 'red': 3, 'too': 3, 'bugs': 3, 'die': 3, 'hard': 3, 'chris': 2, 'poor': 2, 'top': 2, 'war': 2, 'bond': 2, 'perk': 2, 'village': 2, 'having': 2, 'calcutta': 2, 'clerk': 2, 'erin': 2, 'dark': 2, 'foxtrot': 2, 'tango': 2, 'marge': 2, 'matt': 2, 'ooooohh': 2, 'ooo': 2, 'feminist': 2, 'oop': 2, 'total': 2, 'tasty': 2, 'victor': 2, 'peel': 2, 'greg': 2, 'jenny': 2, 'ask': 2, 'eighth': 2, 'dick': 2, 'guru': 2, 'saj': 2, 'tickets': 2, 'women': 2, 'quick': 2, 'judgey': 2, 'thursday': 2, 'keystone': 2, 'caplin': 2, 'wayne': 2, 'ah-ah': 2, 'numb': 2, 'i.': 2, 'victoria': 2, 'personal': 2, 'forgot': 2, 'everyday': 2, 'val': 2, 'kilmer': 2, 'batman': 2, 'anyone': 2, 'hero': 2, 'denise': 2, 'lose': 2, 'danger': 2, 'olivia': 2, 'christian': 2, 'sanders': 2, 'rsvp': 2, 'kurt': 2, 'douglas': 2, 'handling': 2, 'earth': 2, 'said': 2, 'effective': 2, 'harder': 2, 'technically': 2, 'cousin': 2, 'shrill': 2, 'hamilton': 2, 'mount': 2, 'ones': 2, 'vermont': 2, 'today': 2, 'once': 2, 'young': 2, 'cold': 2, 'jester': 2, 'taj': 2, 'mahal': 2, 'dangerous': 2, 'liaisons': 2, 'weekend': 2, 'phase': 2, 'two': 2, 'ruth': 2, 'depending': 2, 'respectfully': 2, 'disagree': 2, 'rocks': 2, 'twice': 2, 'stating': 2, 'become': 2, 'chinese': 2, 'transit': 2, 'authority': 2, 'farrell': 2, 'laundry': 2, 'bigot': 2, 'tests': 2, 'mexico': 2, 'sundays': 2, 'danielle': 2, 'nuh-uh': 2, 'ms': 2, 'inappropriate': 2, 'whazzup': 2, 'molly': 2, 'sometime': 2, 'consider': 2, 'partners': 2, 'busty': 2, 'cover': 2, 'sincerely': 2, 'funyuns': 2, 'nana': 2, 'java': 2, 'favorite': 2, 'returning': 2, 'male': 2, 'character': 2, 'wheeler': 2, 'aquarius': 2, 'gemini': 2, 'taurus': 2, 'virgo': 2, 'sagittarius': 2, 'bernard': 2, 'weirdest': 2, 'ta-da': 2, 'sparkly': 2, 'ice': 2, 'wooo': 2, 'life': 2, 'remembered': 2, 'id': 2, 'jurassic': 2, 'imagine': 2, 'ezels': 2, 'magician': 2, 'box': 2, 'mix': 2, 'up': 2, 'wo': 2, 'a-ha': 2, 'ooooh': 2, 'doorknob': 2, 'waitressing': 2, 'voulez-vous': 2, 'cafe': 2, 'maurice': 2, 'mischa': 2, 'action': 2, 'after': 2, 'russia': 2, 'whatever': 2, 'ho': 2, 'doug': 2, 'a.m': 2, 'tuesday': 2, 'tom': 2, 'cool': 2, 'armadillo': 2, 'american': 2, 'janine': 2, 'charlie': 2, 'july': 2, 'careful': 2, 'light': 2, 'otherwise': 2, 'sid': 2, 'joan': 2, 'collins': 2, 'sandwich': 2, 'cute': 2, 'apollo': 2, 'katie': 2, 'should': 2, 'maria': 2, 'celtics': 2, 'bernice': 2, 'cinderelly': 2, 'empire': 2, 'joseph': 2, 'francis': 2, 'love': 2, 'ultimate': 2, 'fighting': 2, 'champion': 2, 'including': 2, 'score': 2, 'enough': 2, 'sweety': 2, 'bet': 2, 'mary': 2, 'ha-ha-ha': 2, 'sir': 2, 'double': 2, 'an': 2, 'manly': 2, 'avenue': 2, 'nelson': 2, 'liam': 2, 'abott': 2, 'back': 2, 'without': 2, 'latour': 2, 'another': 2, 'chicago': 2, 'illinois': 2, 'stick': 2, 'next': 2, 'try': 2, 'surprise': 2, 'ping': 2, 'hot': 2, 'nine': 2, 'howie': 2, 'things': 2, 'contraction': 2, 'los': 2, 'gary': 2, 'neil': 2, 'yikes': 2, 'heh': 2, 'long': 2, 'obviously': 2, 'betty': 2, 'tall': 2, 'ugly': 2, 'underdog': 2, 'horny': 2, 'patrick': 2, 'instead': 2, 'place': 2, 'time': 2, \"'cause\": 2, 'cobb': 2, 'joanna': 2, 'whitney': 2, 'terry': 2, 'stryker': 2, 'awesome': 2, 'stevie': 2, 'sunday': 2, 'mel': 2, 'probably': 2, 'aaron': 2, 'funny': 2, 'clown': 2, 'jew': 2, 'feels': 2, 'helen': 2, 'annabelle': 2, 'monet': 2, 'andie': 2, 'tooty': 2, 'paleontology': 2, 'glen': 2, 'albany': 2, 'wesley': 2, 'magic': 2, 'alice': 2, 'elaine': 2, 'fortunately': 2, 'martin': 2, 'scorcese': 2, 'comin': 2, 'unfortunately': 2, 'staten': 2, 'rhonda': 2, 'scott': 2, 'used': 2, 'hurry': 2, 'backup': 2, 'coffee': 2, 'morgan': 2, 'chase': 2, 'jennifer': 2, 'howdy': 2, 'kate': 2, 'miller': 2, 'ball': 2, 'welch': 2, 'mia': 2, 'hamm': 2, 'cuba': 2, 'need': 2, 'bird': 2, 'lem': 2, 'petrie': 2, 'huntley': 2, 'scottish': 2, 'simmons': 2, 'churo': 1, 'paulette': 1, 'vincent': 1, 'wonderful': 1, 'judy': 1, 'halloween': 1, 'grasp': 1, 'butternut': 1, 'indians': 1, 'hormones': 1, 'finally': 1, 'william': 1, 'sonoma': 1, 'expect': 1, 'tommy': 1, 'yellow': 1, 'pages': 1, 'merry': 1, 'holiday': 1, 'millennium': 1, 't-shirt': 1, 'empty': 1, 'vase': 1, 'haul': 1, 'ba': 1, 'bernoulli': 1, 'newton': 1, 'aaargh': 1, 'skidmark': 1, 'okay-okay': 1, 'focus': 1, 'fourth': 1, 'haha': 1, 'bicentennial': 1, 'ethan': 1, 'chad': 1, 'terrified': 1, 'puppies': 1, 'unagi': 1, 'captain': 1, 'clark': 1, 'night-night': 1, 'tiffany': 1, 'princess': 1, 'paper': 1, 'understood': 1, 'tequila': 1, 'lusts': 1, 'emma': 1, 'yowza': 1, 'lord': 1, 'doesn': 1, 'y': 1, 'pillman': 1, 'pathetic': 1, 'birthday': 1, 'quack': 1, 'robbie': 1, 'knick': 1, 'ewing': 1, 'sweetheart': 1, 'hating': 1, 'satan': 1, 'yo': 1, 'build': 1, 'finish': 1, 'philly': 1, 'office': 1, 'supplies': 1, 'tennille': 1, 'cilantro': 1, 'reporter': 1, 'k-rock': 1, 'karen': 1, 'talk': 1, 'lights': 1, 'koon': 1, 'doe': 1, 'brazilian': 1, 'stu': 1, 'morse': 1, 'cameron': 1, 'diaz': 1, 'nyu': 1, 'soaps': 1, 'shampoos': 1, 'murray': 1, 'milan': 1, 'l.a': 1, 'lean': 1, 'angel': 1, 'pass': 1, 'google': 1, 'choose': 1, 'gimmie': 1, 'other': 1, 'flirt': 1, 'gone': 1, 'pack': 1, 'explain': 1, 'tramp': 1, 'fran': 1, 'went': 1, 'fruit': 1, 'jam': 1, 'england': 1, 'three': 1, 'celebrities': 1, 'sarandon': 1, 'twenty-one': 1, 'sucker': 1, 'federal': 1, 'la': 1, 'presenting': 1, 'sarah': 1, 'private': 1, 'special': 1, 'stand': 1, 'fraid': 1, 'aaaah': 1, 'intense': 1, 'freeman': 1, 'fair': 1, 'future': 1, 'ouch': 1, 'admit': 1, 'warm': 1, 'paid': 1, 'flying': 1, 'victorian': 1, 'dumb': 1, 'drunken': 1, 'east': 1, 'mississippi': 1, 'stunning': 1, 'gepeto': 1, 'same': 1, 'connect': 1, 'dots': 1, 'clifford': 1, 'burnett': 1, 'date': 1, 'november': 1, 'age': 1, 'cliff': 1, 'muriel': 1, 'lots': 1, 'bagel': 1, 'mail': 1, 'jail': 1, 'bail': 1, 'able': 1, 'pumpkin': 1, 'hardest': 1, 'mazel': 1, 'zinfandel': 1, 'angelica': 1, 'kissey': 1, 'european': 1, 'makes': 1, 'coast': 1, 'guard': 1, 'ritter': 1, 'oon': 1, 'scotty': 1, 'jared': 1, 'paulo': 1, 'closer': 1, 'dakota': 1, 'nude': 1, 'nudes': 1, 'ever': 1, 'item': 1, 'brides': 1, 'donna': 1, 'carin': 1, 'rush': 1, 'miss': 1, 'sergio': 1, 'power': 1, 'bakery': 1, 'clear': 1, 'wrong': 1, 'silence': 1, 'lambs': 1, \"c'mere\": 1, 'er': 1, 'puck': 1, 'aroma': 1, 'room': 1, 'foster': 1, 'iran': 1, 'brother': 1, 'refill': 1, 'don': 1, 'janet': 1, 'fire': 1, 'property': 1, 'reset': 1, 'circle': 1, 'ring': 1, 'mother': 1, 'marks': 1, 'heating': 1, 'radiator': 1, 'heyy': 1, 'screen': 1, 'guild': 1, 'benefits': 1, 'amazing': 1, 'mmm': 1, 'triple': 1, 'freemont': 1, 'local': 1, 'president': 1, 'pie': 1, 'forever': 1, 'windows': 1, 'whenever': 1, 'angeles': 1, 'puzzler': 1, 'high': 1, 'united': 1, 'states': 1, 'reservations': 1, 'simon': 1, 'destiny': 1, 'nooooo': 1, 'wynona': 1, 'rider': 1, 'chan': 1, 'english': 1, 'plural': 1, 'allright': 1, \"'re\": 1, 'abort': 1, 'home': 1, 'ec': 1, 'lot': 1, 'walls': 1, 'testing': 1, 'at-at': 1, 'kick': 1, 'ho-ho-ho': 1, 'broadway': 1, 'appalachia': 1, 'close': 1, 'saucy': 1, 'electrical': 1, 'mister': 1, 'wrestle': 1, 'pat': 1, 'sajak': 1, 'alex': 1, 'trebek': 1, 'woolery': 1, 'oohh': 1, 'thomas': 1, 'ducks': 1, 'trib': 1, 'eric': 1, 'philadelphia': 1, 'find': 1, 'sports': 1, 'international': 1, 'apartment': 1, 'porsch': 1, 'drop': 1, 'eight': 1, 'hanukkah': 1, 'easter': 1, 'woo-hoo': 1, 'japan': 1, 'fisher': 1, 'toulouse': 1, 'nestlé': 1, 'toll': 1, 'house': 1, 'americans': 1, 'patch': 1, 'sisters': 1, 'indeed': 1, 'torme': 1, 'yoo-hoo': 1, 'boobs': 1, 'tim': 1, 'evander': 1, 'holyfield': 1, 'stood': 1, 'eddie': 1, 'moskowitz': 1, 'reaffirms': 1, 'boring': 1, 'uncomfortable': 1, 'crap': 1, 'oranges': 1, 'filing': 1, 'wash': 1, 'sweetie': 1, 'hypothetically': 1, 'lilies': 1, 'foosball': 1, 'oooooh': 1, 'ciao': 1, 'day': 1, 'december': 1, 'oooh': 1, 'jeffrey': 1, 'swing': 1, 'kings': 1, 'carcass': 1, 'francisco': 1, 'riggs': 1, 'lenny': 1, 'note': 1, 'phonetically': 1, 'frontal': 1, 'renaissance': 1, 'caravaggio': 1, 'touch': 1, 'third': 1, 'easy': 1, 'cross-eyed': 1, 'bad': 1, 'rodney': 1, 'lucky': 1, 'storming': 1, 'josephine': 1, 'square': 1, 'hotel': 1, 'during': 1, 'whose': 1, 'shall': 1, 'sells': 1, 'making': 1, 'brooklyn': 1, 'heights': 1, 'cleveland': 1, 'lorraine': 1, 'christmastime': 1, 'quarter': 1, 'dime': 1, 'lint': 1, 'canadian': 1, 'set': 1, 'uff': 1, 'free': 1, 'ahhh': 1, 'much': 1, 'congress': 1, 'waaay': 1, 'donald': 1, 'trump': 1, 'blue': 1, 'he-he': 1, 'smooth': 1, 'muse': 1, 'madison': 1, 'cathedral': 1, 'thai': 1, 'slow': 1, 'ancient': 1, 'egypt': 1, 'mesopotamia': 1, 'byzantine': 1, 'fabulous': 1, 'deep': 1, 'impact': 1, 'like-like': 1, 'count': 1, 'rules': 1, 'dehydrated': 1, 'japanese': 1, 'question': 1, 'shelley': 1, 'perhaps': 1, 'g.i': 1, 'u.s.': 1, 'down': 1, 'debbie': 1, 'charla': 1, 'know': 1, 'dan': 1, 'barley': 1, 'someone': 1, 'mugsy': 1, 'canada': 1, 'felicity': 1, 'brady': 1, 'men': 1, 'whoooaa': 1, 'milwaukee': 1, 'disneyland': 1, 'small': 1, 'dutch': 1, 'kingdom': 1, 'colonial': 1, 'rick': 1, 'greenpeace': 1, 'crew': 1, 'mri': 1, 'dna': 1, 'clearly': 1, 'ho-oh': 1, 'café': 1, 'knowing': 1, 'kiss': 1, 'vail': 1, 'thanksgivings': 1, 'year': 1, 'irony': 1, 'smell': 1, 'phd': 1, 'tho': 1, 'sting': 1, 'ummm': 1, 'amanda': 1, 'rue': 1, 'keep': 1, 'sudden': 1, 'seems': 1, 'lift': 1, 'carl': 1, 'gawd': 1, 'married': 1, 'junior': 1, 'nancy': 1, 'thompson': 1, 'sleepy': 1, 'cutie': 1, 'burt': 1, 'off-broadway': 1, 'organic': 1, 'starlight': 1, 'his': 1, 'phillips': 1, 'division': 1, 'ted': 1, 'teddy': 1, 'andrew': 1, 'chuckles': 1, 'works': 1, 'sugar': 1, 'smacks': 1, 'boyfriend': 1, 'nor': 1, 'gibson': 1, 'clint': 1, 'eastwood': 1, 'ph.d': 1, 'huge': 1, 'sweet': 1, 'anything': 1, 'limited': 1, 'gellar': 1, 'blood': 1, 'coming': 1, 'monroe': 1, 'sensitive': 1, 'hulk': 1, 'pbs': 1, 'telethon': 1, 'half': 1, 'bummer': 1, 'make': 1, 'again': 1, 'pregnant': 1, 'woman': 1, 'slays': 1, 'dead': 1, 'devon': 1, 'brolin': 1, 'russell': 1, 'school': 1, 'restaurant': 1, 'minister': 1, 'everyone': 1, 'luckily': 1, 'crazy': 1, 'seven': 1, 'bambi': 1, 'folks': 1, 'unbound': 1, 'nora': 1, 'tyler': 1, \"ma'am\": 1, 'until': 1, 'book': 1, 'save': 1, 'cancel': 1, 'show': 1, 'trudie': 1, 'won': 1, 'fantastic': 1, 'bunny': 1, \"d'you\": 1, 'learned': 1, 'acting': 1, 'beginners': 1, 'tiki': 1, 'death': 1, 'punch': 1, 'weeeell': 1, 'drum': 1, 'help': 1, 'barney': 1, 'tweezers': 1, 'manhattan': 1, 'face': 1, 'rome': 1, 'vienna': 1, 'mcclane': 1, 'friends': 1, 'las': 1, 'dice': 1, 'pay': 1, 'beth': 1, 'adiós': 1, 'flowers': 1, 'evil': 1, 'nova': 1, 'scotia': 1, 'order': 1, 'beans': 1, 'dowdy': 1, 'mira': 1, 'mannequins': 1, 'momma': 1, 'whistle': 1, 'spoon': 1, 'thata': 1, 'human': 1, 'olé': 1, 'eva': 1, 'gay': 1, 'mikey': 1, 'p': 1, 'setting': 1, 'suzie': 1, 'do-do': 1, 'front': 1, 'lebanon': 1, 'hand': 1, 'hanks': 1, 'meg': 1, 'ryan': 1, 'f.y.i': 1, 'stephanie': 1, 'awwwww': 1, 'quite': 1, 'at': 1, 'bastard': 1, 'linda': 1, 'doing': 1, 'phone': 1, 'its': 1, 'braverman': 1, 'kicky': 1, 'fettuccini': 1, 'alfredo': 1, 'cheese': 1, 'hate': 1, 'gon': 1, 'stitches': 1, 'postpone': 1, 'anywhere': 1, 'bobo': 1, 'leon': 1, 'gotcha': 1, 'vcr': 1, 'street': 1, 'mama': 1, 'enh': 1, 'section': 1, 'health': 1, 'code': 1, 'nimitz': 1, 'chief': 1, 'valentine': 1, 'food': 1, 'stephanopoulos': 1, 'strip': 1, 'winona': 1, 'ryder': 1, 'michelle': 1, 'dorothy': 1, 'hammel': 1, 'fonzie': 1, 'doogie': 1, 'officer': 1, 'petty': 1, 'wonder': 1, 'santos': 1, 'oh-kay': 1, 'forty-five': 1, 'places': 1, 'turn': 1, 'zelda': 1, 'scotland': 1, 'vikings': 1, 'pillow': 1, 'fight': 1, 'g': 1, 'string': 1, 'kitten': 1, 'grandfather': 1, 'swedish': 1, 'grandmother': 1, 'allan': 1, 'theta': 1, 'beta': 1, 'pi': 1, 'syracuse': 1, 'hoshi': 1, 'zana': 1, 'devane': 1, 'kelly': 1, 'goldie': 1, 'mcguire': 1, 'j.t.': 1, 'beardsley': 1, 'dorfman': 1, 'shame': 1, 'stanley': 1, 'wh-what': 1})\n"
     ]
    }
   ],
   "source": [
    "known_words_count = Counter(known_words)\n",
    "print('Number of known words',len(known_words_count))\n",
    "print('Number of known words tokens:', len(known_words))\n",
    "print('Unknown words counts')\n",
    "print(known_words_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as in the previous notebook, we need to turn the labels into numerical values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']\n",
      "[4, 4, 4, 4, 6, 4, 4, 4, 4, 4, 2, 4, 6, 4, 6, 5, 6, 2, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "# first we instantiate a label encode\n",
    "le = preprocessing.LabelEncoder()\n",
    "# we fee this encoder with the complete list of labels from our data\n",
    "le.fit(training_labels)\n",
    "print(list(le.classes_))\n",
    "training_classes = le.transform(training_labels)\n",
    "print(list(training_classes[0:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next steps are the same as for the previous notebook, except that we pass the embedding representations of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "# from sklearn.cross_validation import train_test_split  # deprecated in 0.18\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "### we again use a aplit of 80% train and 20% test\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    trainDataVecs, # the tf-idf model\n",
    "    training_classes, # the category values for each utterance represented as numeric values\n",
    "    test_size = 0.20 # we use 80% for training and 20% for development\n",
    "    ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 2 ... 3 1 3]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(y_test)\n",
    "print(type(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.1636000e-01 -5.9840000e-01  4.1309002e-01 -7.6799989e-03\n",
      "  -7.3045099e-01 -2.8516501e-01  8.5350150e-01  6.8652004e-01\n",
      "  -9.8149502e-01 -2.5839502e-01 -1.4743000e-01 -2.9082000e-01\n",
      "  -2.4461000e+00 -5.2369499e-01  3.7858999e-01 -2.2097851e-01\n",
      "  -4.2551702e-01 -5.3156996e-01 -5.5166495e-01  2.1673501e-01\n",
      "   3.6101121e-01 -3.5664991e-02 -4.6165699e-01  3.5926002e-01\n",
      "   2.5298998e-01]\n",
      " [ 2.1999502e-01 -1.2061520e-01  1.8493812e-01 -2.6136104e-02\n",
      "  -1.7570910e-01  7.2638109e-02  9.3399107e-01  3.5766679e-01\n",
      "  -1.5275399e-01  2.5836399e-01  5.0035994e-02 -7.0398413e-02\n",
      "  -4.0326300e+00 -3.6890402e-01  3.5035007e-02  1.5320063e-03\n",
      "   2.3311678e-01 -3.4084767e-01 -4.6753740e-01 -3.7266392e-01\n",
      "   1.3768899e-01  2.2526228e-01  1.7695587e-02  3.5733521e-01\n",
      "  -2.0955701e-01]\n",
      " [-2.6078999e-01  5.9108001e-01  6.1622000e-01 -7.0367998e-01\n",
      "  -8.5158998e-01 -2.3238000e-01  1.0481000e+00  6.6642001e-02\n",
      "  -5.4907000e-01  7.0046997e-01 -8.7221003e-01 -1.3954000e-02\n",
      "  -5.9671001e+00 -4.3105999e-01 -9.1540003e-01  5.3744000e-01\n",
      "   5.7099003e-01 -2.7181000e-01 -8.4178001e-01 -5.9682000e-01\n",
      "   4.5159999e-01  3.4097001e-01  7.6869003e-02  2.2840001e-01\n",
      "   2.7579999e-01]\n",
      " [-6.9521412e-02 -4.8315994e-02  3.5611844e-01 -2.8394315e-01\n",
      "  -3.9080444e-01  3.2288715e-01  9.5945007e-01  5.5405772e-01\n",
      "  -5.1735288e-01  6.4351571e-01 -2.9352000e-01 -1.0527687e-01\n",
      "  -4.5007005e+00 -2.0545329e-01 -3.1004572e-01  1.5221085e-01\n",
      "   2.9314715e-01 -3.8166872e-01 -6.9380128e-01 -6.2851286e-01\n",
      "   2.0844099e-01  4.0918571e-01 -3.7854174e-01  2.3878002e-01\n",
      "   2.3788571e-01]\n",
      " [ 1.2694490e-01 -4.2332226e-01  7.8172892e-02  4.6264995e-02\n",
      "  -2.1565677e-01  3.3701068e-01  1.1074345e+00  6.4185607e-01\n",
      "  -2.4819779e-01  1.9462000e-01 -2.2199999e-01 -2.3805554e-01\n",
      "  -3.8429663e+00 -3.6097556e-01  2.4377888e-01 -2.6091254e-01\n",
      "  -1.0623666e-01 -3.2003427e-01 -5.3465188e-01 -4.0169767e-01\n",
      "   5.4716662e-02  2.8119063e-01  3.6365543e-02  6.5918279e-01\n",
      "   1.9064254e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(docs_train[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training and applying the model  <a class=\"anchor\" id =\"section4\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piek/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/piek/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/piek/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/piek/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/piek/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/piek/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/piek/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/piek/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/piek/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/piek/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CalibratedClassifierCV(base_estimator=LinearSVC(C=1.0, class_weight=None,\n",
       "                                                dual=True, fit_intercept=True,\n",
       "                                                intercept_scaling=1,\n",
       "                                                loss='squared_hinge',\n",
       "                                                max_iter=1000,\n",
       "                                                multi_class='ovr', penalty='l2',\n",
       "                                                random_state=None, tol=0.0001,\n",
       "                                                verbose=0),\n",
       "                       cv=10, method='sigmoid')"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "linear_model = svm.LinearSVC()\n",
    "svm_linear_clf = CalibratedClassifierCV(linear_model , method='sigmoid', cv=10)\n",
    "svm_linear_clf.fit(docs_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results, find macro recall\n",
    "y_pred_svm_linear = svm_linear_clf.predict(docs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data is complex, a non-linear SVM may be preferable. A non-linear SVM uses the kernel-trick to separate positive and negative cases when the data is not lineary correlated. We can initialise such a classifier in the same way as done before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CalibratedClassifierCV(base_estimator=SVC(C=1.0, break_ties=False,\n",
       "                                          cache_size=200, class_weight=None,\n",
       "                                          coef0=0.0,\n",
       "                                          decision_function_shape='ovr',\n",
       "                                          degree=3, gamma='scale', kernel='rbf',\n",
       "                                          max_iter=-1, probability=True,\n",
       "                                          random_state=None, shrinking=True,\n",
       "                                          tol=0.001, verbose=False),\n",
       "                       cv=10, method='sigmoid')"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonlinear_model = svm.SVC(probability=True)\n",
    "svm_nonlinear_clf = CalibratedClassifierCV(nonlinear_model,  method='sigmoid', cv=10)\n",
    "svm_nonlinear_clf.fit(docs_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results, find macro recall\n",
    "y_pred_svm_nonlinear = svm_nonlinear_clf.predict(docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05988991 0.01646456 0.02898296 ... 0.51481068 0.04868501 0.25063108]\n",
      " [0.12300721 0.02939583 0.02557653 ... 0.53107906 0.07739238 0.03355987]\n",
      " [0.16838638 0.01655129 0.02917025 ... 0.51575326 0.04934721 0.19957302]\n",
      " ...\n",
      " [0.11907268 0.04358776 0.01963189 ... 0.51676135 0.0490766  0.07569682]\n",
      " [0.12051742 0.04387402 0.01992854 ... 0.52771971 0.08041836 0.03196055]\n",
      " [0.03609401 0.01718546 0.03027797 ... 0.53499558 0.12118331 0.09195121]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_svm_nonlinear_proba= svm_nonlinear_clf.predict_proba(docs_test)\n",
    "print(y_pred_svm_nonlinear_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generating the test report  <a class=\"anchor\" id =\"section5\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger' 'disgust' 'fear' 'joy' 'neutral' 'sadness' 'surprise']\n",
      "SVM LINEAR ----------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0  0.4230769 0.0940171 0.1538462       234\n",
      "           1  0.5000000 0.0384615 0.0714286        52\n",
      "           2  0.0000000 0.0000000 0.0000000        53\n",
      "           3  0.5130435 0.3361823 0.4061962       351\n",
      "           4  0.5345477 0.9072495 0.6727273       938\n",
      "           5  0.0000000 0.0000000 0.0000000       132\n",
      "           6  0.3949580 0.1974790 0.2633053       238\n",
      "\n",
      "    accuracy                      0.5205205      1998\n",
      "   macro avg  0.3379466 0.2247699 0.2239291      1998\n",
      "weighted avg  0.4506927 0.5205205 0.4384254      1998\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piek/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(y_test,y_pred_svm_linear,digits = 7)\n",
    "print(le.classes_)\n",
    "print('SVM LINEAR ----------------------------------------------------------------')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger' 'disgust' 'fear' 'joy' 'neutral' 'sadness' 'surprise']\n",
      "SVM NONLINEAR ----------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0  0.4117647 0.0299145 0.0557769       234\n",
      "           1  0.0000000 0.0000000 0.0000000        52\n",
      "           2  0.0000000 0.0000000 0.0000000        53\n",
      "           3  0.4791667 0.3276353 0.3891709       351\n",
      "           4  0.5366889 0.9434968 0.6841902       938\n",
      "           5  0.0000000 0.0000000 0.0000000       132\n",
      "           6  0.4891304 0.1890756 0.2727273       238\n",
      "\n",
      "    accuracy                      0.5265265      1998\n",
      "   macro avg  0.2738215 0.2128746 0.2002665      1998\n",
      "weighted avg  0.4426265 0.5265265 0.4285937      1998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test,y_pred_svm_nonlinear,digits = 7)\n",
    "print(le.classes_)\n",
    "print('SVM NONLINEAR ----------------------------------------------------------------')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Remember the results from the notebook where we trained a NaiveBayes and SVM classifiers with one-hot-encodings of the words? Take some time to compare the results and think about the differences.\n",
    "\n",
    "We show here some screen dumps:\n",
    "\n",
    "\n",
    "* One-hot-encoding, frequency = 1, 2000 training documents\n",
    "\n",
    "![one-hot-token-results-nb-svm-2000-f=1.png](attachment:one-hot-token-results-nb-svm-2000-f=1.png)\n",
    "\n",
    "* One-hot-encoding, frequency = 5, 2000 training documents\n",
    "\n",
    "![one-hot-token-results-nb-svm-2000-f=5.png](attachment:one-hot-token-results-nb-svm-2000-f=5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Applying the model to new data  <a class=\"anchor\" id =\"section6\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to apply the embedding based model to our own data but this works a bit different as we cannot simply use the 'transform' function to represent the utterances using the one-hot vector representation of the training vocabulary.\n",
    "\n",
    "What we need to do is to create an embedding representation using the same function we used above and assume that our classifier finds sufficient similarity in the embeddings of our data with the correct training data.\n",
    "\n",
    "We use the same set of utterances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some utterances\n",
    "some_chat = ['That is sweet of you', \n",
    "               'You are so funny', \n",
    "               'Are you a man or a woman?', \n",
    "               'Chatbots make me sad and feel lonely.', \n",
    "               'Your are stupid and boring.', \n",
    "               'Two thumbs up', \n",
    "               'I fell asleep halfway through this conversation', \n",
    "               'Wow, I am really amazed.', \n",
    "               'You are amazing.']\n",
    "\n",
    "\n",
    "len(some_chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the list of labels that go with our chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_chat_emotions = ['joy', 'joy', 'neutral', 'sadness', 'anger', 'joy', 'anger', 'surprise', 'joy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We  use the LabelEncoder *le* to convert this list into a numpy array with digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels ['anger' 'disgust' 'fear' 'joy' 'neutral' 'sadness' 'surprise']\n",
      "[3 3 4 5 0 3 0 6 3]\n"
     ]
    }
   ],
   "source": [
    "print('labels',le.classes_)\n",
    "some_chat_labels = le.transform(some_chat_emotions)\n",
    "print(some_chat_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piek/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "some_chat_tokens = []\n",
    "for utterance in some_chat:\n",
    "    some_chat_tokens.append(nltk.tokenize.word_tokenize(utterance))\n",
    "\n",
    "some_chat_embedding_vectors = getAvgFeatureVecs(some_chat_tokens, word_embedding_model, num_features)\n",
    "#### Due to the averaging, there could be infinitive values or NaN values. The next numpy function turns these value to \"0\" scores\n",
    "some_chat_embedding_vectors = np.nan_to_num(some_chat_embedding_vectors)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System predictions [3 3 4 4 4 4 4 4 3]\n",
      "Gold labels [3 3 4 5 1 3 1 6 3]\n",
      "That is sweet of you => joy\n",
      "You are so funny => joy\n",
      "Are you a man or a woman? => neutral\n",
      "Chatbots make me sad and feel lonely. => neutral\n",
      "Your are stupid and boring. => neutral\n",
      "Two thumbs up => neutral\n",
      "I fell asleep halfway through this conversation => neutral\n",
      "Wow, I am really amazed. => neutral\n",
      "You are amazing. => joy\n"
     ]
    }
   ],
   "source": [
    "# have classifier make a prediction\n",
    "#pred = svm_linear_clf.predict(ourDataVecs)\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "some_chat_pred = svm_linear_clf.predict(some_chat_embedding_vectors)\n",
    "print('System predictions', some_chat_pred)\n",
    "print('Gold labels', some_chat_labels_np)\n",
    "for review, predicted_label in zip(some_chat, some_chat_pred):\n",
    "    \n",
    "    print('%s => %s' % (review, \n",
    "                        le.classes_[predicted_label]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05566912 0.03380374 0.0149815  0.30998191 0.25843324 0.04503507\n",
      "  0.28209541]\n",
      " [0.14537836 0.04532041 0.02102572 0.34213409 0.13950034 0.04908878\n",
      "  0.25755229]\n",
      " [0.114386   0.05677569 0.03045086 0.14124956 0.51209787 0.07597918\n",
      "  0.06906083]\n",
      " [0.13183531 0.08699858 0.04358781 0.26563304 0.34602976 0.0825597\n",
      "  0.0433558 ]\n",
      " [0.14198635 0.06169215 0.02710553 0.08070763 0.57224455 0.03777203\n",
      "  0.07849176]\n",
      " [0.27403921 0.02139631 0.03342049 0.0716617  0.53701941 0.0357632\n",
      "  0.02669968]\n",
      " [0.24249596 0.04499757 0.078444   0.05729908 0.46323222 0.08313668\n",
      "  0.03039449]\n",
      " [0.0375989  0.03000299 0.02195037 0.21628631 0.60877046 0.05640656\n",
      "  0.02898442]\n",
      " [0.06568752 0.0380387  0.00887373 0.58062801 0.20605358 0.02699314\n",
      "  0.07372531]]\n"
     ]
    }
   ],
   "source": [
    "some_chat_pred_probabilities = svm_linear_clf.predict_proba(some_chat_embedding_vectors)\n",
    "print(some_chat_pred_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using *Pandas* we can nicely visualise the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anger</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>neutral</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>Chat</th>\n",
       "      <th>Predication</th>\n",
       "      <th>Gold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.566912</td>\n",
       "      <td>3.380374</td>\n",
       "      <td>1.498150</td>\n",
       "      <td>30.998191</td>\n",
       "      <td>25.843324</td>\n",
       "      <td>4.503507</td>\n",
       "      <td>28.209541</td>\n",
       "      <td>That is sweet of you</td>\n",
       "      <td>joy</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.537836</td>\n",
       "      <td>4.532041</td>\n",
       "      <td>2.102572</td>\n",
       "      <td>34.213409</td>\n",
       "      <td>13.950034</td>\n",
       "      <td>4.908878</td>\n",
       "      <td>25.755229</td>\n",
       "      <td>You are so funny</td>\n",
       "      <td>joy</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.438600</td>\n",
       "      <td>5.677569</td>\n",
       "      <td>3.045086</td>\n",
       "      <td>14.124956</td>\n",
       "      <td>51.209787</td>\n",
       "      <td>7.597918</td>\n",
       "      <td>6.906083</td>\n",
       "      <td>Are you a man or a woman?</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.183531</td>\n",
       "      <td>8.699858</td>\n",
       "      <td>4.358781</td>\n",
       "      <td>26.563304</td>\n",
       "      <td>34.602976</td>\n",
       "      <td>8.255970</td>\n",
       "      <td>4.335580</td>\n",
       "      <td>Chatbots make me sad and feel lonely.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.198635</td>\n",
       "      <td>6.169215</td>\n",
       "      <td>2.710553</td>\n",
       "      <td>8.070763</td>\n",
       "      <td>57.224455</td>\n",
       "      <td>3.777203</td>\n",
       "      <td>7.849176</td>\n",
       "      <td>Your are stupid and boring.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>27.403921</td>\n",
       "      <td>2.139631</td>\n",
       "      <td>3.342049</td>\n",
       "      <td>7.166170</td>\n",
       "      <td>53.701941</td>\n",
       "      <td>3.576320</td>\n",
       "      <td>2.669968</td>\n",
       "      <td>Two thumbs up</td>\n",
       "      <td>neutral</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>24.249596</td>\n",
       "      <td>4.499757</td>\n",
       "      <td>7.844400</td>\n",
       "      <td>5.729908</td>\n",
       "      <td>46.323222</td>\n",
       "      <td>8.313668</td>\n",
       "      <td>3.039449</td>\n",
       "      <td>I fell asleep halfway through this conversation</td>\n",
       "      <td>neutral</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.759890</td>\n",
       "      <td>3.000299</td>\n",
       "      <td>2.195037</td>\n",
       "      <td>21.628631</td>\n",
       "      <td>60.877046</td>\n",
       "      <td>5.640656</td>\n",
       "      <td>2.898442</td>\n",
       "      <td>Wow, I am really amazed.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6.568752</td>\n",
       "      <td>3.803870</td>\n",
       "      <td>0.887373</td>\n",
       "      <td>58.062801</td>\n",
       "      <td>20.605358</td>\n",
       "      <td>2.699314</td>\n",
       "      <td>7.372531</td>\n",
       "      <td>You are amazing.</td>\n",
       "      <td>joy</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       anger   disgust      fear        joy    neutral   sadness   surprise  \\\n",
       "0   5.566912  3.380374  1.498150  30.998191  25.843324  4.503507  28.209541   \n",
       "1  14.537836  4.532041  2.102572  34.213409  13.950034  4.908878  25.755229   \n",
       "2  11.438600  5.677569  3.045086  14.124956  51.209787  7.597918   6.906083   \n",
       "3  13.183531  8.699858  4.358781  26.563304  34.602976  8.255970   4.335580   \n",
       "4  14.198635  6.169215  2.710553   8.070763  57.224455  3.777203   7.849176   \n",
       "5  27.403921  2.139631  3.342049   7.166170  53.701941  3.576320   2.669968   \n",
       "6  24.249596  4.499757  7.844400   5.729908  46.323222  8.313668   3.039449   \n",
       "7   3.759890  3.000299  2.195037  21.628631  60.877046  5.640656   2.898442   \n",
       "8   6.568752  3.803870  0.887373  58.062801  20.605358  2.699314   7.372531   \n",
       "\n",
       "                                              Chat Predication      Gold  \n",
       "0                             That is sweet of you         joy       joy  \n",
       "1                                 You are so funny         joy       joy  \n",
       "2                        Are you a man or a woman?     neutral   neutral  \n",
       "3            Chatbots make me sad and feel lonely.     neutral   sadness  \n",
       "4                      Your are stupid and boring.     neutral   disgust  \n",
       "5                                    Two thumbs up     neutral       joy  \n",
       "6  I fell asleep halfway through this conversation     neutral   disgust  \n",
       "7                         Wow, I am really amazed.     neutral  surprise  \n",
       "8                                 You are amazing.         joy       joy  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_chat_pred_labels = []\n",
    "for predicted_label in some_chat_pred:\n",
    "    some_chat_pred_labels.append(le.classes_[predicted_label])\n",
    "\n",
    "some_chat_gold_labels = []\n",
    "for gold_label in some_chat_labels_np:\n",
    "    some_chat_gold_labels.append(le.classes_[gold_label])\n",
    "\n",
    "\n",
    "result_frame = pd.DataFrame(some_chat_pred_probabilities*100, columns=le.classes_)\n",
    "result_frame['Chat']=some_chat\n",
    "result_frame['Predication']=some_chat_pred_labels\n",
    "result_frame['Gold']=some_chat_gold_labels\n",
    "\n",
    "result_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger' 'disgust' 'fear' 'joy' 'neutral' 'sadness' 'surprise']\n",
      "SVM LINEAR ----------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1  0.0000000 0.0000000 0.0000000         2\n",
      "           3  1.0000000 0.7500000 0.8571429         4\n",
      "           4  0.1666667 1.0000000 0.2857143         1\n",
      "           5  0.0000000 0.0000000 0.0000000         1\n",
      "           6  0.0000000 0.0000000 0.0000000         1\n",
      "\n",
      "    accuracy                      0.4444444         9\n",
      "   macro avg  0.2333333 0.3500000 0.2285714         9\n",
      "weighted avg  0.4629630 0.4444444 0.4126984         9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piek/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(some_chat_labels_np,some_chat_pred,digits = 7)\n",
    "print(le.classes_)\n",
    "print('SVM LINEAR ----------------------------------------------------------------')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Saving the classifier to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as with the previous notebook, you can save the emotion classification model to disk and load the model some other time. Note that you need to load the same word2vec model as well to represent any text input with vector representations that are compatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save the classifier to disk\n",
    "filename_classifier = 'svm_nonlinear_clf_embeddings.sav'\n",
    "pickle.dump(svm_nonlinear_clf, open(filename_classifier, 'wb'))\n",
    " \n",
    "# some time later...\n",
    " \n",
    "# load the classifier and the vectorizer from disk\n",
    "loaded_classifier = pickle.load(open(filename_classifier, 'rb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
