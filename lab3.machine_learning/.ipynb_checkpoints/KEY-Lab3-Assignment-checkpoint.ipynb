{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KEY Assignment 2 about NERC (max 20 points for your final grade)\n",
    "\n",
    "This notebook describes Assignment 3, which is part of Lab 3 of the text mining course. \n",
    "\n",
    "\n",
    "**Learning goals**\n",
    "* going from linguistic input format to representing it in a feature space\n",
    "* working with pretrained word embeddings\n",
    "* train a supervised classifier (SVM)\n",
    "* evaluate a supervised classifier (SVM)\n",
    "* perform feature ablation and gain insight into the contribution of various features\n",
    "\n",
    "We assume you have worked through the following notebook:\n",
    "* **Lab3-Supervised NERC system.ipynb**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Points: 3] Exercise 1: NER and NERC definitions\n",
    "* **[1 point] a) Explain what NER is**\n",
    "* **[1 point] b) Explain what NERC is**\n",
    "* **[1 point] c) Explain what the IOB format is and how it represents both the NER and NERC task.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "* NER is a text mining task of recognizing/marking entity mentions in text.\n",
    "* NERC is a text mining task of recognizing/marking entity mentions in text and classifying them as one of the predefined entity types (e.g., person or location).\n",
    "* The IOB format assigns a label to each token, as one of the following: O (meaning not an entity), B-* (meaning first token of an entity mention), or I-* (meaning non-first token of an entity mention). In the above, the asterisk '\\*' should be replaced with an entity type, such as PER or LOC. This allows annotation of mentions as well as their class in the same label. An example label would be B-LOC, for a first token of some location entity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Points: 17] Exercise 2: Training and evaluating an SVM using CoNLL-2003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 point] a) Load the CoNLL-2003 training data using the *ConllCorpusReader* and create for both *train.txt* and *test.txt*:**\n",
    "\n",
    "    [2 points]  -a list of dictionaries representing the features for each training instances, e..g,\n",
    "    ```\n",
    "    [\n",
    "    {'words': 'EU', 'pos': 'NNP'}, \n",
    "    {'words': 'rejects', 'pos': 'VBZ'},\n",
    "    ...\n",
    "    ]\n",
    "    ```\n",
    "\n",
    "    [2 points] -the NERC labels associated with each training instance, e.g.,\n",
    "    dictionaries, e.g.,\n",
    "    ```\n",
    "    [\n",
    "    'B-ORG', \n",
    "    'O',\n",
    "    ....\n",
    "    ]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'word': 'EU', 'pos': 'NNP'}, {'word': 'rejects', 'pos': 'VBZ'}, {'word': 'German', 'pos': 'JJ'}, {'word': 'call', 'pos': 'NN'}, {'word': 'to', 'pos': 'TO'}, {'word': 'boycott', 'pos': 'VB'}, {'word': 'British', 'pos': 'JJ'}, {'word': 'lamb', 'pos': 'NN'}, {'word': '.', 'pos': '.'}, {'word': 'Peter', 'pos': 'NNP'}]\n",
      "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'B-PER']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus.reader import ConllCorpusReader\n",
    "\n",
    "train = ConllCorpusReader('nerc_datasets/CONLL2003', 'train.txt', ['words', 'pos', 'ignore', 'chunk'])\n",
    "training_features = []\n",
    "training_gold_labels = []\n",
    "\n",
    "for token, pos, ne_label in train.iob_words():\n",
    "    a_dict = {\n",
    "       'word':token,\n",
    "        'pos':pos,\n",
    "    }\n",
    "    training_features.append(a_dict)\n",
    "    training_gold_labels.append(ne_label)\n",
    "    \n",
    "print(training_features[:10])\n",
    "print(training_gold_labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'word': 'SOCCER', 'pos': 'NN'}, {'word': '-', 'pos': ':'}, {'word': 'JAPAN', 'pos': 'NNP'}, {'word': 'GET', 'pos': 'VB'}, {'word': 'LUCKY', 'pos': 'NNP'}, {'word': 'WIN', 'pos': 'NNP'}, {'word': ',', 'pos': ','}, {'word': 'CHINA', 'pos': 'NNP'}, {'word': 'IN', 'pos': 'IN'}, {'word': 'SURPRISE', 'pos': 'DT'}]\n",
      "['O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "test = ConllCorpusReader('nerc_datasets/CONLL2003', 'test.txt', ['words', 'pos', 'ignore', 'chunk'])\n",
    "\n",
    "test_features = []\n",
    "test_gold_labels = []\n",
    "for token, pos, ne_label in test.iob_words():\n",
    "\n",
    "    a_dict = {\n",
    "       'word':token,\n",
    "        'pos':pos,\n",
    "    }\n",
    "    test_features.append(a_dict)\n",
    "    test_gold_labels.append(ne_label)\n",
    "    \n",
    "print(test_features[:10])\n",
    "print(test_gold_labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[3 points] b) provide descriptive statistics about the training and test data:**\n",
    "* How many instances are in train and test?\n",
    "* Provide a frequency distribution of the NERC labels, i.e., how many times does each NERC label occur?\n",
    "* Discuss to what extent the training and test data is balanced (equal amount of instances for each NERC label) and to what extent the training and test data differ?\n",
    "\n",
    "Tip: you can use the following `Counter` functionality to generate frequency list of a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples 203621\n",
      "Number of test examples 46435\n"
     ]
    }
   ],
   "source": [
    "print('Number of training examples', len(training_gold_labels))\n",
    "print('Number of test examples', len(test_gold_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'O': 169578, 'B-LOC': 7140, 'B-PER': 6600, 'B-ORG': 6321, 'I-PER': 4528, 'I-ORG': 3704, 'B-MISC': 3438, 'I-LOC': 1157, 'I-MISC': 1155})\n",
      "Counter({'O': 38323, 'B-LOC': 1668, 'B-ORG': 1661, 'B-PER': 1617, 'I-PER': 1156, 'I-ORG': 835, 'B-MISC': 702, 'I-LOC': 257, 'I-MISC': 216})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter \n",
    "\n",
    "print(Counter(training_gold_labels))\n",
    "print(Counter(test_gold_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "The training data and the test data predominantly (say around 80%) consist of 'O' labels, whih is expected since most phrases typically do not refer to entities.\n",
    "\n",
    "The training data is 4-5 times larger that the test data which is also the case for the amount of some of the labels like B-LOC. While the ratio and the frequency rank for other labels might vary, overall we observe a similar distribution in the training and the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[1 point] c. Explain what one hot encoding is. Explain why we can not use string features to train the SVM.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "One hot encoding is a sparse representation of the features that mostly contains zeros and occasional ones. Every feature value in the training data corresponds to one position in the one hot vector.\n",
    "\n",
    "The string features need to be encoded as numbers because machine learning systems require numeric input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[3 points] d. Concatenate the train and test features (the list of dictionaries) into one list. Load it using the *DictVectorizer*. Afterwards, split it back to training and test.**\n",
    "\n",
    "Tip: You’ve concatenated train and test into one list and then you’ve applied the DictVectorizer.\n",
    "The order of the rows is maintained. You can hence use an index (number of training instances) to split the_array back into train and test. Do NOT use: `\n",
    "from sklearn.model_selection import train_test_split` here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-20f6acf24d98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDictVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mthe_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_features\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtest_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mthe_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthe_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtraining_onehot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthe_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_features' is not defined"
     ]
    }
   ],
   "source": [
    "vec = DictVectorizer()\n",
    "the_array = training_features + test_features\n",
    "the_array = vec.fit_transform(the_array)\n",
    "\n",
    "training_onehot = the_array[:len(training_features)]\n",
    "test_onehot = the_array[len(training_features):]\n",
    "the_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 points] e. Train the SVM using the train features and labels and evaluate on the test data. Provide a classification report (sklearn.metrics.classification_report).**\n",
    "The train (*lin_clf.fit*) might take a while. On my computer, it took 1min 53s, which is acceptable. Training models normally takes much longer. If it takes more than 5 minutes, you can use a subset for training. Describe the results:\n",
    "* Which NERC labels does the classifier perform well on? Why do you think this is the case?\n",
    "* Which NERC labels does the classifier perform poorly on? Why do you think this is the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_clf = svm.LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      B-LOC      0.812     0.775     0.793      1668\n",
      "     B-MISC      0.782     0.664     0.718       702\n",
      "      B-ORG      0.792     0.519     0.627      1661\n",
      "      B-PER      0.860     0.437     0.579      1617\n",
      "      I-LOC      0.618     0.529     0.570       257\n",
      "     I-MISC      0.570     0.588     0.579       216\n",
      "      I-ORG      0.703     0.467     0.561       835\n",
      "      I-PER      0.333     0.871     0.481      1156\n",
      "          O      0.985     0.984     0.985     38323\n",
      "\n",
      "avg / total      0.939     0.920     0.922     46435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lin_clf.fit(training_onehot,training_gold_labels)\n",
    "y_pred = lin_clf.predict(test_onehot)\n",
    "report = classification_report(test_gold_labels,y_pred,digits = 3)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word': 'BRUSSELS', 'pos': 'NNP'}\n",
      "B-LOC\n",
      "{'word': 'Germany', 'pos': 'NNP'}\n",
      "B-LOC\n",
      "{'word': 'Britain', 'pos': 'NNP'}\n",
      "B-LOC\n",
      "{'word': 'Britain', 'pos': 'NNP'}\n",
      "B-LOC\n",
      "{'word': 'France', 'pos': 'NNP'}\n",
      "B-LOC\n",
      "{'word': 'France', 'pos': 'NNP'}\n",
      "B-LOC\n",
      "{'word': 'Britain', 'pos': 'NNP'}\n",
      "B-LOC\n",
      "{'word': 'Europe', 'pos': 'NNP'}\n",
      "B-LOC\n",
      "{'word': 'Germany', 'pos': 'NNP'}\n",
      "B-LOC\n",
      "{'word': 'Bonn', 'pos': 'NNP'}\n",
      "B-LOC\n",
      "{'word': 'Germany', 'pos': 'NNP'}\n",
      "B-LOC\n",
      "{'word': 'Britain', 'pos': 'NNP'}\n",
      "B-LOC\n",
      "{'word': 'LONDON', 'pos': 'NNP'}\n",
      "B-LOC\n",
      "{'word': 'U.S.', 'pos': 'NNP'}\n",
      "B-LOC\n",
      "{'word': 'Florida', 'pos': 'NNP'}\n",
      "B-LOC\n",
      "{'word': 'London', 'pos': 'NNP'}\n",
      "B-LOC\n",
      "{'word': 'Nottingham', 'pos': 'NNP'}\n",
      "B-LOC\n",
      "{'word': 'China', 'pos': 'NNP'}\n",
      "B-LOC\n",
      "{'word': 'Taiwan', 'pos': 'NNP'}\n",
      "B-LOC\n",
      "{'word': 'BEIJING', 'pos': 'VBG'}\n",
      "B-LOC\n",
      "{'word': 'China', 'pos': 'NNP'}\n",
      "B-LOC\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for w,l in zip(training_features,training_gold_labels):\n",
    "    if 'LOC' in l:\n",
    "        print(w)\n",
    "        print(l)\n",
    "        i+=1\n",
    "    if i>20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 points] f. Train a model that uses the embeddings of these words as inputs. Test again on the same data as in 2e. Generate a classification report and compare the results with the classifier you built in 2e.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('./model/GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs(data, model):\n",
    "    inputs=[]\n",
    "    for token, pos, ne_label in data.iob_words():\n",
    "        if token in model:\n",
    "            vector=model[token]\n",
    "        else: # if the word does not exist in the embeddings vocabulary, use an all-zeros vector\n",
    "            vector=[0]*300\n",
    "            #print('not in vocabulary:', token)\n",
    "        inputs.append(vector)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_inputs=get_inputs(train, model)\n",
    "test_inputs=get_inputs(test,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC      0.759     0.801     0.779      1668\n",
      "      B-MISC      0.724     0.695     0.709       702\n",
      "       B-ORG      0.690     0.638     0.663      1661\n",
      "       B-PER      0.746     0.669     0.705      1617\n",
      "       I-LOC      0.514     0.424     0.465       257\n",
      "      I-MISC      0.604     0.537     0.569       216\n",
      "       I-ORG      0.480     0.332     0.392       835\n",
      "       I-PER      0.586     0.501     0.540      1156\n",
      "           O      0.973     0.991     0.982     38323\n",
      "\n",
      "   micro avg      0.927     0.927     0.927     46435\n",
      "   macro avg      0.675     0.621     0.645     46435\n",
      "weighted avg      0.921     0.927     0.923     46435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "lin_clf = svm.LinearSVC()\n",
    "\n",
    "lin_clf.fit(training_inputs,training_gold_labels)\n",
    "y_pred = lin_clf.predict(test_inputs)\n",
    "report = classification_report(test_gold_labels,y_pred,digits = 3)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
