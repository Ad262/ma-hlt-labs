{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab3.3 Machine learning using embeddings\n",
    "Introduction to HLT, RMA VU University\n",
    "\n",
    "In this notebook, you are going to use word embeddings instead of the one-hot-encoding of words. Word embeddings have many advantages:\n",
    "\n",
    "* they capture similarities across words that can be learned from massive amounts of text data without annotation\n",
    "* machine learning can easily exploit these similarity because the embeddings are also represented as vectors\n",
    "* the word embedding vectors are much smaller (30 up to 500 dimensions) and more dense than one-hot-encodings, which results in more efficient and compact models that can also generalize better\n",
    "\n",
    "At the end of this notebook, you should have learned:\n",
    "\n",
    "* how replace the words in your training set by there embeddings\n",
    "* how to train a classifier enriched with embeddings\n",
    "* how to represent the words for any unseen text as embeddings\n",
    "* how to add embeddings to our NERC system\n",
    "* how to work with some popular data sets for NERC to which such embeddings ca be applied\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Quick introduction to embeddings\n",
    "\n",
    "Extracting features manually can get us a long way. In addition to lemma and part-of-speech, people have used a huge number of other information: features of the previous words (on the left) or the next words (on the right), whether the current word starts with a capital, whether it is an abbreviation, etc.\n",
    "\n",
    "A recent alternative way to create a 'semantic' representation of a word is by word embeddings: mapping words (or phrases) from the vocabulary to vectors of real numbers. Conceptually it involves a mathematical embedding from a space with many dimensions per word to a continuous vector space with a much lower dimension. For this reason, they are called dense representations.\n",
    "\n",
    "In linguistics, word embeddings were discussed in the research area of distributional semantics. The idea is to quantify and categorize semantic similarities between linguistic items based on their distributional properties in large samples of language data. The underlying notion is that \"a word is characterized by the company it keeps\" (Firth)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will load pre-trained word embeddings called word2vec, created by Google. \n",
    "\n",
    "First, please download the file from [their google drive](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit). Then, create a folder in the same directory as this notebook, called 'model' and unpack the word2vec file in that folder.\n",
    "\n",
    "We will load the embedding model with the Gensim package that we used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now load the file using the gensim library (this takes a while):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_model = gensim.models.KeyedVectors.load_word2vec_format('./model/GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings capture certain meaning aspects of words. Previous research has shown that they can partially capture simiarity (\"tapas\" is similar to \"pintxos\"), relatedness (tapas relates to Spain), and analogy (\"Paris\" is to \"France\" as \"Rome\" is to \"Italy\"). \n",
    "\n",
    "To get an idea of these properties of embeddings, we can compute the cosine similarity between two word vectors. We will expect for example, that \"cat\" and \"tiger\" are more similar than \"cat\" and \"Germany\". Feel free to play a bit with word1 and word2 below to get some feeling of the information these embeddings capture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6477412]]\n"
     ]
    }
   ],
   "source": [
    "word1='tapas'\n",
    "word2='pintxos'\n",
    "word1_vector=np.array(word_embedding_model[word1]).reshape(1, -1)\n",
    "word2_vector=np.array(word_embedding_model[word2]).reshape(1, -1)\n",
    "print(cosine_similarity(word1_vector, word2_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the most similar words to some word, say 'apple':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('apples', 0.7203598022460938), ('pear', 0.6450696587562561), ('fruit', 0.6410146355628967), ('berry', 0.6302294731140137), ('pears', 0.6133961081504822), ('strawberry', 0.6058261394500732), ('peach', 0.6025873422622681), ('potato', 0.596093475818634), ('grape', 0.5935864448547363), ('blueberry', 0.5866668224334717)]\n"
     ]
    }
   ],
   "source": [
    "print(word_embedding_model.most_similar('apple', topn=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Using embeddings in our NERC model\n",
    "\n",
    "Next, we will use the same example of Named Entity Recognition and Classification (NERC) as before but now replace the one-hot-vector for the vocabularies by their dense embeddings.\n",
    "\n",
    "We use the same example as before and process it using SpaCy. We define the labels in the same way as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "text=\"Germany's representative to the European Union\"\n",
    "\n",
    "doc=nlp(text)\n",
    "\n",
    "## The series of labels that go with the word tokens from the input text\n",
    "y=['B-LOC', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now replace the one-hot input representation of our words with embeddings. We generate our input data by simply looking up each word in the embeddings model.\n",
    "\n",
    "The following code creates an array from all the tokens in the document object \"doc\" by taking the embedding vectors for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This word is not in the word2vec vocabulary: 's\n",
      "This word is not in the word2vec vocabulary: to\n"
     ]
    }
   ],
   "source": [
    "training_input=[]\n",
    "for token in doc:\n",
    "    word=token.text  #the next word from the tokenized text\n",
    "    # we check if our model (loaded with the Google word2vec embeddings)\n",
    "    # is inside the model\n",
    "    if word in word_embedding_model:\n",
    "        # in this case the word was found and vector is assigned with its embedding vector as the value\n",
    "        vector=word_embedding_model[word]\n",
    "    else: \n",
    "        # if the word does not exist in the embeddings vocabulary, \n",
    "        # we create a vector with all zeros.\n",
    "        # The Google word2vec model has 300 dimensions so we creat a vector with 300 zeros\n",
    "        vector=[0]*300\n",
    "        print('This word is not in the word2vec vocabulary:', word)\n",
    "    training_input.append(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the first element in our training_input, which is the same size as the tokenized sentence but the words are replaced by embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the training input =  7\n",
      "[ 0.25976562  0.140625    0.24707031  0.00958252 -0.25       -0.08251953\n",
      " -0.09912109 -0.35351562 -0.1484375   0.1484375  -0.03540039 -0.05249023\n",
      "  0.09277344 -0.14257812 -0.01483154  0.01647949  0.03710938  0.18847656\n",
      " -0.03955078 -0.05786133  0.26757812  0.10693359 -0.04345703  0.06738281\n",
      " -0.00177765  0.1328125  -0.16308594 -0.05908203 -0.22558594  0.12207031\n",
      "  0.10791016 -0.19433594 -0.16210938 -0.14257812  0.09033203 -0.14648438\n",
      " -0.12109375  0.09960938  0.26367188  0.12695312  0.140625    0.11083984\n",
      "  0.02697754 -0.01635742  0.00292969  0.14746094 -0.06542969 -0.16699219\n",
      "  0.03662109  0.14941406 -0.14746094  0.06835938 -0.09228516  0.12207031\n",
      " -0.09179688  0.09082031 -0.38476562  0.03051758 -0.21679688 -0.12597656\n",
      " -0.08642578 -0.26171875 -0.08496094 -0.13964844 -0.02832031 -0.203125\n",
      "  0.29101562 -0.13574219 -0.07226562  0.16308594 -0.19042969  0.22265625\n",
      "  0.05566406  0.21289062  0.05053711 -0.09814453  0.12158203  0.01000977\n",
      "  0.15234375 -0.02233887  0.07324219 -0.27148438 -0.01977539 -0.07128906\n",
      "  0.13964844 -0.06542969  0.08886719  0.01452637  0.03344727  0.12158203\n",
      " -0.0703125  -0.02282715 -0.15332031  0.22265625 -0.22265625 -0.18164062\n",
      "  0.08642578 -0.26367188 -0.03735352  0.04321289  0.03039551 -0.19042969\n",
      "  0.07080078  0.37695312 -0.01318359 -0.02526855 -0.00775146  0.02868652\n",
      " -0.00350952 -0.08203125  0.15234375 -0.20996094  0.16796875  0.05322266\n",
      "  0.13476562  0.05102539  0.00454712  0.02099609  0.37890625  0.24707031\n",
      " -0.25585938 -0.27148438  0.0402832  -0.00604248 -0.11865234 -0.05957031\n",
      "  0.14550781 -0.265625   -0.47851562  0.02709961  0.03833008  0.02441406\n",
      " -0.1640625  -0.19921875 -0.08984375 -0.16015625 -0.06079102 -0.03491211\n",
      "  0.2109375   0.03637695  0.20898438 -0.00588989  0.015625    0.10986328\n",
      "  0.08154297  0.11328125  0.296875    0.30664062 -0.12890625 -0.14550781\n",
      "  0.11474609  0.02258301 -0.02905273 -0.08544922 -0.06201172 -0.08740234\n",
      " -0.21777344  0.10986328  0.14550781  0.21679688  0.07177734  0.02294922\n",
      "  0.13964844 -0.15917969  0.22265625 -0.03515625 -0.03515625  0.07763672\n",
      " -0.12695312  0.25       -0.24707031  0.04345703  0.05786133 -0.1171875\n",
      " -0.11425781 -0.29492188  0.11523438 -0.16992188 -0.05078125 -0.09228516\n",
      "  0.04101562 -0.3984375  -0.24707031 -0.19238281  0.4375      0.12988281\n",
      "  0.12890625  0.03271484 -0.22558594  0.20800781  0.03564453 -0.06982422\n",
      "  0.12792969 -0.00300598  0.17285156  0.15722656  0.12890625 -0.16210938\n",
      " -0.24121094 -0.3984375   0.02246094 -0.06176758  0.12792969  0.02526855\n",
      "  0.01867676 -0.01745605 -0.15722656 -0.06201172 -0.22460938 -0.02087402\n",
      "  0.00192261 -0.17382812  0.09521484  0.23046875 -0.16992188 -0.22363281\n",
      "  0.23632812  0.05126953  0.13867188 -0.00238037 -0.05664062 -0.2890625\n",
      " -0.11767578 -0.11767578 -0.28710938 -0.0324707   0.04150391 -0.08447266\n",
      "  0.1953125  -0.24121094 -0.19433594 -0.04418945  0.0559082   0.01293945\n",
      "  0.05444336  0.12988281 -0.02502441  0.19726562 -0.17675781 -0.15234375\n",
      "  0.21484375  0.00515747  0.01269531  0.11132812 -0.02722168  0.15625\n",
      " -0.20800781 -0.02416992 -0.11621094 -0.08789062 -0.02075195  0.01782227\n",
      "  0.00439453  0.18261719 -0.19433594  0.26367188  0.04467773  0.01293945\n",
      "  0.15917969  0.0859375   0.12988281  0.00141907  0.02441406  0.25976562\n",
      "  0.28320312  0.11669922 -0.03564453 -0.25       -0.0559082   0.29296875\n",
      "  0.04833984 -0.03393555  0.13769531 -0.19140625  0.21875    -0.08300781\n",
      " -0.10058594  0.20117188 -0.11425781 -0.00595093 -0.06982422 -0.046875\n",
      " -0.06591797  0.02453613 -0.27148438  0.19628906  0.22265625  0.02429199\n",
      "  0.04394531  0.10253906 -0.06079102  0.11816406 -0.06445312  0.11376953\n",
      " -0.17285156  0.171875   -0.19433594  0.00817871  0.06835938 -0.14160156]\n"
     ]
    }
   ],
   "source": [
    "print(\"The length of the training input = \",len(training_input))\n",
    "print(training_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as in the earlier cases, once we have the vector representations, we can use them to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "lin_clf = svm.LinearSVC()\n",
    "lin_clf.fit(training_input, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing the model** Let's say we want to test our model with the sentence: 'I love beer from Munich'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O' 'O' 'O' 'O' 'B-LOC']\n"
     ]
    }
   ],
   "source": [
    "test_sentence='I love beer from Munich'\n",
    "test_doc=nlp(test_sentence)\n",
    "gold_labels=['O', 'O', 'O', 'O', 'B-LOC']\n",
    "\n",
    "test_inputs=[]\n",
    "\n",
    "for token in test_doc:\n",
    "    word=token.text\n",
    "    if word in word_embedding_model:\n",
    "        vector=word_embedding_model[word]\n",
    "    else:\n",
    "        vector=[0]*300\n",
    "    test_inputs.append(vector)\n",
    "    \n",
    "pred=lin_clf.predict(test_inputs)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have now trained and testing your first embeddings-based NERC model.\n",
    "\n",
    "As mentioned above, a more modern version of this model would be to replace SVM with a sequence-to-sequence architecture from the recurrent neural networks family.\n",
    "\n",
    "So far you have just worked with a few toy examples. In order to obtain a good performance machine learning systems may need thousands and sometimes hundreds of thousands training examples. The vocabulary of a language is large and there is also large variation in expressions. Having only a few examples for each words or expression requires to have massive amounts of text.\n",
    "\n",
    "To some extent, word embedding resolve the issue of *data sparseness*, as words unseen in the training data may still be similar to other words that arein the training data. Word embeddings are derived from millions of documents (billions of tokens) and are likely to have embeddings for most common words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2') Combining embeddings with one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vec = DictVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_instances=[]\n",
    "for token in doc:\n",
    "    one_training_instance={'part-of-speech': token.pos_, 'lemma': token.lemma_} # this concatenates the PoS and Lemma\n",
    "    training_instances.append(one_training_instance)\n",
    "\n",
    "the_array = vec.fit_transform(training_instances).toarray() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 12)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_array.shape\n",
    "# ROWS are WORDS, COLUMNS are FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This word is not in the word2vec vocabulary: 's\n",
      "This word is not in the word2vec vocabulary: to\n"
     ]
    }
   ],
   "source": [
    "training_input=[]\n",
    "for token in doc:\n",
    "    word=token.text  #the next word from the tokenized text\n",
    "    # we check if our model (loaded with the Google word2vec embeddings)\n",
    "    # is inside the model\n",
    "    if word in word_embedding_model:\n",
    "        # in this case the word was found and vector is assigned with its embedding vector as the value\n",
    "        vector=word_embedding_model[word]\n",
    "    else: \n",
    "        # if the word does not exist in the embeddings vocabulary, \n",
    "        # we create a vector with all zeros.\n",
    "        # The Google word2vec model has 300 dimensions so we creat a vector with 300 zeros\n",
    "        vector=[0]*300\n",
    "        print('This word is not in the word2vec vocabulary:', word)\n",
    "    training_input.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 300)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(training_input).shape\n",
    "# ROWS are WORDS, COLUMNS are EMBEDDINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now concatenate the features for each word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-499faccd8d66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeatures_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthe_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0membeddings_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "features_input=np.array(the_array)\n",
    "embeddings_input=np.array(training_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features_input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7313c2172fa9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeatures_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'features_input' is not defined"
     ]
    }
   ],
   "source": [
    "features_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "assert features_input.shape[0]==embeddings_input.shape[0], \n",
    "'Error: different number of words are represented by embeddings compared to linguistic features'\n",
    "\n",
    "num_words=features_input.shape[0]\n",
    "concat_input=[]\n",
    "for index in range(num_words):\n",
    "    print(index)\n",
    "    representation=list(features_input[index]) + list(embeddings_input[index]) # concatenate features per word\n",
    "    concat_input.append(representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 312)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(concat_input).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. NERC datasets\n",
    "\n",
    "In this section, we will look at large data sets that have been created by the community in which people have been annotating entities. In the assignment (Lab4.5.assignment) you will use this data to train and test models on this data that give a realistic performance.\n",
    "\n",
    "Here, we will load two NERC datasets and quickly inspect their contents.\n",
    "\n",
    "**Preparation** Please download the .zip file with the two datasets from [this link](http://kyoto.let.vu.nl/~vossen/rma_hlt/nerc_datasets.zip)\n",
    "\n",
    "Then unpack the .zip, so that the folder `nerc_datasets` lies in the same directory as this notebook.\n",
    "\n",
    "### 3.1 CoNLL-2003\n",
    "\n",
    "Now that we've seen how to represent linguistic features, we also need to access relevant linguistic training data for the NERC task. One of the most popular datasets is [CoNLL-2003](http://aclweb.org/anthology/W03-0419), which was provided with the zip file you just downloaded.\n",
    "You can load it using the following code snippet, which makes use of the NLTK function ConllCorpusReader to do the magic. More information on the ConllCorpusReader can be found here: https://www.nltk.org/_modules/nltk/corpus/reader/conll.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader import ConllCorpusReader\n",
    "\n",
    "train = ConllCorpusReader('nerc_datasets/CONLL2003', # the folder where ConLL-2003 is stored (you downloaded this with the zip file from canvas) \n",
    "                          'train.txt', # this will load the file 'train.txt', for the exercise you also need to load 'test.xt' \n",
    "                          ['words', 'pos', 'ignore', 'chunk'])\n",
    "for token, pos, ne_label in train.iob_words():\n",
    "    print(token, pos, ne_label) # please represent this information using a dictionary for the feature representation\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can for example iterate through this data, and make a list of the tokens as inputs, and of the `ne_label` values as desirable outputs. The input tokens could for example be looked up in our word embeddings dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vectors=[]\n",
    "labels=[]\n",
    "for token, pos, ne_label in train.iob_words():\n",
    "    \n",
    "    if token!='' and token!='DOCSTART':\n",
    "        if token in word_embedding_model:\n",
    "            vector=word_embedding_model[token]\n",
    "        else:\n",
    "            vector=[0]*300\n",
    "        input_vectors.append(vector)\n",
    "        labels.append(ne_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully loaded our data. Let's see how many tokens/labels we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Last ten labels =', labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, we should have the same size of input_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(input_vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a next step, we could easily train a model on this data as shown in above by combining the input vectors with the labels in a fir function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Kaggle\n",
    "Another interesting dataset is the [Annotated Corpus for Named Entity Recognition](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus), which we also provided in the zip file you downloaded from Canvas. You can load it in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'nerc_datasets/kaggle/ner_v2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_dataset = pandas.read_csv(path, error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see the following output after running the above code cell:\n",
    "```\n",
    "b'Skipping line 281837: expected 25 fields, saw 34\\n'\n",
    "```\n",
    "You can ignore this.\n",
    "\n",
    "**pandas.read_csv** will load the csv file into a [pandas DataFrame](https://towardsdatascience.com/pandas-dataframe-a-lightweight-intro-680e3a212b96)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can inspect which columns are in the csv file by running the following code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can seen that a wide range of features is given for each token. [Here](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus), you can read what each column represents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You loop can loop through the dataset in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, instance in kaggle_dataset.iterrows():\n",
    "    print()\n",
    "    print(index)\n",
    "    print(instance) # you can access information by using instance['A COLUMN NAME'] which you can use to convert to a dictionary needed for the feature representation.\n",
    "    print('NERC label', instance['tag'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could for instance use these features as inputs in a machine learning model with our DictVectorizer, or by transforming them using embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of this notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
