{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning basics\n",
    "\n",
    "This notebook explains the simple basics of machine learning. At the end of this notebook, you learned:\n",
    "\n",
    "- the basic principles of machine learning \n",
    "- how features are represented as vectors\n",
    "- how to train a classifier from vector representations\n",
    "- how to train and apply a classifier to text represented by its words\n",
    "- what a bag-of-words representation is\n",
    "- what the information value (TF*IDF) of a word is\n",
    "\n",
    "**Background reading:**\n",
    "\n",
    "NLTK Book\n",
    "Chapter 6, section 1 and 3: https://www.nltk.org/book/ch06.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Machine Learning schema\n",
    "\n",
    "The overall process of machine learning is shown in the next image that is taken from Chapter 6 of the NLTK book. In general, machine learning consists of a training phase in which an algorithm associates data features with certain labels (e.g. sentiment, part-of-speech). The training results in a classifier model that can be applied to unseen data. The classifier compares the features of the unseen data with the previously seen data and makes a prediction of the label on the basis of some similarity calculation.\n",
    "\n",
    "![title](images/ml-schema.pdf)\n",
    "\n",
    "\n",
    "Crucial in this process is 1) the features that represent the data and 2) the algorithm that is used. In this course, we are not going to discuss the various machine learning algorithm in depth but we focus on the text features and how they are represented as so-called vectors. In the case of a text, we need to define what the features are that charcaterize the text. These features are transformed into a feature vector representation that the algorithm and model can handle. In order to compare the unseen text with the training texts, it is crucial that features are extracted and represented in the same way across training and applying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparations**\n",
    "\n",
    "We are going to use the Scikit-learn package to transform the diverse feature values into a vector representation:\n",
    "\n",
    "https://scikit-learn.org/stable/install.html\n",
    "\n",
    "Scikit-learn is a package that contains a lot of machine learning algorithms and functions for dealing with features and carrying out evaluation and error analysis. To install it run one of the following commands from the command line:\n",
    "\n",
    "- pip install -U scikit-learn\n",
    "\n",
    "or\n",
    " \n",
    "- conda install scikit-learn\n",
    "\n",
    "We are also using a package called \"numpy\": https://numpy.org.\n",
    "\n",
    "Install \"numpy\" from the command line following the instructions on the website. After installing, you can import it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Vector representations\n",
    "\n",
    "\n",
    "Before we turn to a text example, we are going to use a very simple data set. We show how to train and evaluate an SVM (Support-Vector-Machine) using a made-up example of multi-class classification for a non-linguistic dataset. The goal is to predict someone's weight category (say: skinny, fit, average, overweight) based on their properties.\n",
    "\n",
    "We use three features:\n",
    "* **age in years**\n",
    "* **height in cms**\n",
    "* **number of ice cream cones eaten per year**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature representation (for 5 people) is an array of arrays. Each instance (or person) is represented by an array of numbers in which the first is the age, the second the heights in cms and the third the number of cones per year: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[30, 180, 1000], \n",
    "     [80, 180, 100],\n",
    "     [50, 180, 100],\n",
    "     [40, 160, 500],\n",
    "     [15, 160, 400]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first person is thus 30 years old, 180 cms tall and eats 1000 cones per year. The next command prints the data for the first instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First instance in the data set X = [30, 180, 1000]\n"
     ]
    }
   ],
   "source": [
    "print('First instance in the data set X =', X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An array of numbers in which each position holds a value for a specific feature is what we call a feature vector. For all our data in the data set we must have a feature vector of the same length. If there is no value, it will be zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the data that is now assigned to the variable 'X', we also need to have the prediction that goes with the instances. For this we use another array with the values that we assign to the variable 'Y'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = [\"overweight\", \n",
    "     \"skinny\",\n",
    "     \"fit\",\n",
    "     \"average\",\n",
    "     \"average\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to have as many values as we have instances in our data set, as the software pairs the elements in X with the elements in Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the data set = 5\n",
      "The length of the predictions = 5\n",
      "The first prediction = overweight\n"
     ]
    }
   ],
   "source": [
    "print('The length of the data set =', len(X))\n",
    "print('The length of the predictions =', len(Y))\n",
    "print('The first prediction =', Y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Using Skikit learn to build a classifier\n",
    "\n",
    "Now we have the data and the prediction we can train a model. We are going to use the **svm** module from **sklearn**, from which we will select the **LinearSVR** (Linear Support Vector Regression) class. For now it is not important to know the details about this algorithm. You will learn about that in the machine learning class. We instantiate a model with the variable name 'lin_classifier' (any name will do). We will use this instantiation for training and classifying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "lin_classifier = svm.LinearSVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the model by feeding it with the data set 'X' and the predictions 'Y'. Feeding we do with the 'fit' function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_classifier.fit(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model. You might get a warning stating that:\n",
    "```\n",
    "ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
    "```\n",
    "This is to be expected given that we only train using five instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Using Skikit learn to classify unseen data\n",
    "\n",
    "Let's now apply the model to a new instance 'Z': what does SVM think the weight category is of someone of 18 years, 171cm, and who eats 400 ice cream cones per year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['average']\n"
     ]
    }
   ],
   "source": [
    "Z=[[18, 171, 400]]\n",
    "predicted_label = lin_classifier.predict(Z)\n",
    "print(predicted_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently the SVM thinks it is **average**, which is not surprising since **number of ice cream cones eaten per year** and **height** seem to correlate highly with the weight categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Representing a text as a Bag-Of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A critical component of almost any machine learning approach is **feature representation**. \n",
    "This is not strange since we need to somehow convert a textual unit, e.g., word, sentence, tweet, or document, into something meaningful that can not only be interpreted by a computer, but is also useful for the type of learning we want to do. \n",
    "\n",
    "A text consists of a sequence of words on which we impose syntax and semantics. A machine needs to learn to associate the structural properties of the text to some interpretation.\n",
    "We can use various properties to do this:\n",
    "\n",
    "- the words (regardless of the order)\n",
    "- the words and their frequency\n",
    "- the part-of-speech of words\n",
    "- word pairs\n",
    "- the characters that make up the words\n",
    "- sentences with words\n",
    "- phrases\n",
    "- the meaning of words\n",
    "- the meaning of combinations of words\n",
    "- etc....\n",
    "\n",
    "Some of the above properties, we get for free if we split a text into tokens (the words), e.g. by using spaces. Still, we need to consider what to do with punctuation and how to treate upper/lower cases (the word shape). Other properties are not explicit, such as the part-of-speech of words, phrases, syntax and the meaning.\n",
    "\n",
    "For now, we are only considering the words of a text as features. In fact, we are going to ignore the order of the words and consider a text as a *Bag-Of-Words*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you want to learn more: (information from these blogs was used in this notebook)**\n",
    "* [bag of words introduction](http://www.insightsbot.com/blog/R8fu5/bag-of-words-algorithm-in-python-introduction)\n",
    "* [TF-IDF introduction](https://medium.freecodecamp.org/how-to-process-textual-data-using-tf-idf-in-python-cd2bbc0a94a3)\n",
    "* [another TF-IDF introduction](https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/)\n",
    "\n",
    "In the next notebook of this course, we explain how other features can be combined with a word representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Bag of words\n",
    "The bag of word approach consists of two main steps that result in a word-to-document index:\n",
    "\n",
    "* 1 we extract all the unique word from a collections of textual units, e.g., documents\n",
    "* 2 we compute the frequency of each word in each document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this for the following three sentences that we list in an array (note that sentences can also be complete documents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = ['A rose is a rose',\n",
    "         'A rose is a flower',\n",
    "         \"A book is nice\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the **CountVectorizer** to create the bag of words representation from the above array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can adapt min_df to restrict the representation to more frequent words e.g. 2, 3, etc..\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=1, # in how many documents the term minimally occurs\n",
    "                             tokenizer=nltk.word_tokenize) # we use the nltk tokenizer to split the text into tokens\n",
    "sents_counts = vectorizer.fit_transform(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the so-called \"shape\" of sents_counts shows us that we have 3 documents and 6 unique words spread over these documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 6)\n",
      "unique words: ['a', 'rose', 'is', 'flower', 'book', 'nice']\n"
     ]
    }
   ],
   "source": [
    "# sents_counts has a dimension of 3 (document count) by 6 (# of unique words)\n",
    "print(sents_counts.shape)\n",
    "print('unique words:', list(vectorizer.vocabulary_.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bag of word representation for the text data looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary of all the sentences  consists of the followng words: ['a', 'book', 'flower', 'is', 'nice', 'rose']\n",
      "The vector representation of the sentences looks as follows:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 0, 1, 0, 2],\n",
       "       [2, 0, 1, 1, 0, 1],\n",
       "       [1, 1, 0, 1, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this vector is small enough to view in full! \n",
    "print('The vocabulary of all the sentences  consists of the following words:', vectorizer.get_feature_names())\n",
    "\n",
    "print('The vector representation of the sentences looks as follows:')\n",
    "sents_counts.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks familiar (think about the age, length and cones data set we have seen before). What happened to each sentence representation?\n",
    "\n",
    "The first array has 6 positions representing the complete vocabulary. The first position represents the first word \"a\" and it has value '2', which means it occurs twice in the sentence. The fourth slot is for \"is\" which occurs once and the sixth slot is for \"rose\" which occurs twice. The other slots are zero because these words do not occur in the first sentence.\n",
    "\n",
    "Try to figure out if you understand the representation of the other two sentences!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 TF-IDF\n",
    "One big problem of the bag of words approach is that it treats all words equally. Why is that a disadvantage? It means that words that occur in many documents, such as *A,* contribute equally to the decision making of the machine learning approach as other words that are much more informative, e.g., *rose*. \n",
    "TF-IDF addresses this problem by assigning less weight to words that occur in many documents.\n",
    "You read [here](https://medium.freecodecamp.org/how-to-process-textual-data-using-tf-idf-in-python-cd2bbc0a94a3) a nice introduction to TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how you can do it in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "sents_tfidf = tfidf_transformer.fit_transform(sents_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'book', 'flower', 'is', 'nice', 'rose']\n",
      "[[0.6 0.  0.  0.3 0.  0.8]\n",
      " [0.6 0.  0.5 0.3 0.  0.4]\n",
      " [0.4 0.6 0.  0.4 0.6 0. ]]\n"
     ]
    }
   ],
   "source": [
    "tf_idf_array = sents_tfidf.toarray()\n",
    "print(vectorizer.get_feature_names())\n",
    "print(numpy.round(tf_idf_array, decimals=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a good result! In the bag of words approach, The words **\"a\"** and **\"book\"** both had a frequency of 1 in the third sentence. Now that we've applied the TF-IDF approach, we see that the word *book* has a higher weight (0.6) than the word *\"a\"* since *\"a\"* occurs in all three sentences and *\"book\"* only in one, which might indicate that it is more informative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Training a classifier with word vectors\n",
    "Now we have seen how we can turn a text into a vector representation. We can associate these text representation to labels as we have seen above for predicting somebody's weight. We now use different labels but note that for the algorithm the labels are meaningless. They could be numbers of any label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels= [\"tautology\", \n",
    "     \"hyponymy\",\n",
    "     \"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_classifier_count = svm.LinearSVC()\n",
    "lin_classifier_count.fit(sents_counts,labels)\n",
    "\n",
    "lin_classifier_weight = svm.LinearSVC()\n",
    "lin_classifier_weight.fit(tf_idf_array,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Representing a new text using the word vectors from the training vocabulary\n",
    "\n",
    "Next, we can use the same vocabulary to make a new sentence and represent it in the same way as the training example. This means we can only represent words that have been observed during training and we ignore new words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words from the training data: ['a', 'rose', 'is', 'flower', 'book', 'nice']\n"
     ]
    }
   ],
   "source": [
    "print('Unique words from the training data:', list(vectorizer.vocabulary_.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize as tok\n",
    "new_text=\"a good book is a rose\"\n",
    "\n",
    "# the vector representation using the vocabulary of the training data \n",
    "# would look as follows:\n",
    "new_text_vector=[[2, 1, 0, 1, 0, 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the word \"good\" is not represented as it does not occur in the vocabulary. We cannot compare this feature with the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sentiment']\n"
     ]
    }
   ],
   "source": [
    "predicted_label = lin_classifier_count.predict(new_text_vector)\n",
    "print(predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tautology']\n"
     ]
    }
   ],
   "source": [
    "predicted_label = lin_classifier_weight.predict(new_text_vector)\n",
    "print(predicted_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of this notebooks. Please continue with notebook Lab4.2.ml.linguistic_features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
