{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab3.5a Testing classifiers on a different data set with tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The data set that we are going to use is from a NLP task on emotion detection that was organised in the *Wassa* workshop in 2017. The texts are tweets and therefore a different genre than the spoken utterances from the conversations in the MED data set:\n",
    "\n",
    "http://saifmohammad.com/WebPages/EmotionIntensity-SharedTask.html\n",
    "\n",
    "We included the data set in the distribution of this lab and aggregated all the training data in a single file that we now can load using *Pandas*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading the tweet data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Label</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000</td>\n",
       "      <td>How the fu*k! Who the heck! moved my fridge!.....</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>So my Indian Uber driver just called someone t...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10002</td>\n",
       "      <td>@DPD_UK I asked for my parcel to be delivered ...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10003</td>\n",
       "      <td>so ef whichever butt wipe pulled the fire alar...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10004</td>\n",
       "      <td>Don't join @BTCare they put the phone down on ...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                              Tweet  Label  Score\n",
       "0  10000  How the fu*k! Who the heck! moved my fridge!.....  anger  0.938\n",
       "1  10001  So my Indian Uber driver just called someone t...  anger  0.896\n",
       "2  10002  @DPD_UK I asked for my parcel to be delivered ...  anger  0.896\n",
       "3  10003  so ef whichever butt wipe pulled the fire alar...  anger  0.896\n",
       "4  10004  Don't join @BTCare they put the phone down on ...  anger  0.896"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "filepath = './data/wassa/training/all.train.tsv'\n",
    "dftweets = pd.read_csv(filepath, sep='\\t')\n",
    "dftweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3613 entries, 0 to 3612\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   ID      3613 non-null   int64  \n",
      " 1   Tweet   3613 non-null   object \n",
      " 2   Label   3613 non-null   object \n",
      " 3   Score   3613 non-null   float64\n",
      "dtypes: float64(1), int64(1), object(2)\n",
      "memory usage: 113.0+ KB\n"
     ]
    }
   ],
   "source": [
    "dftweets.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this data set has 3613 tweets, labels and a score. Lets check the distribution of the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fc1a1702790>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dftweets.Label.value_counts().sort_values(ascending=False).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this data set is more evenly distributed and there are no neutral tweets. Also lacking are *disgust* and *surprise*. Let's see what our classifier does on this data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first extract the tweets and labels using a similar loop as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3613\n",
      "3613\n"
     ]
    }
   ],
   "source": [
    "tweet_instances=[]\n",
    "tweet_labels = []\n",
    "for tweet in dftweets['Tweet']:\n",
    "    ### We break the loop after 2000 instances \n",
    "    #if index==2000:\n",
    "    #    break\n",
    "    tweet_instances.append(tweet)\n",
    "\n",
    "\n",
    "for label in dftweets['Label']:\n",
    "    ### We break the loop after 2000 instances \n",
    "    #if index==2000:\n",
    "    #    break    ### we need to surround the next statements with 'try' and 'except' to catch cases \n",
    "    tweet_labels.append(label)\n",
    "\n",
    "print(len(tweet_instances))\n",
    "print(len(tweet_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Representing and classifying the tweets as averaged word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Representing the tweets using the same word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the same functions that we used in the notebook *Lab3.4a.ml.emotion-detection.embeddings.ipynb* to get averaged embeddings for the tweets. We repeat these functions in this notebook.\n",
    "\n",
    "Note that the tweets have special tokens such as hashtags, emoticons. In so far these are not part of the embedding vocabulary, they will not be accounted for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_words =[]\n",
    "known_words = []\n",
    "# Function to average all word vectors in a paragraph\n",
    "def featureVecMethod(words, stopwords, model, modelword_index, num_features):\n",
    "    # Pre-initialising empty numpy array for speed\n",
    "    # This create a numpy array with the length of the num_features set to zero values\n",
    "    featureVec = np.zeros(num_features,dtype=\"float32\")\n",
    "    nwords = 0\n",
    "        \n",
    "    for word in  words:\n",
    "        if not word in stop_words: \n",
    "            if word in index2word_set:\n",
    "                nwords = nwords + 1\n",
    "                featureVec = np.add(featureVec,model[word])\n",
    "            else:\n",
    "                word = word.lower()\n",
    "                if word in index2word_set:\n",
    "                    nwords = nwords + 1\n",
    "                    featureVec = np.add(featureVec,model[word])\n",
    "                    #we keep track of the words detected\n",
    "                    known_words.append(word)\n",
    "                else:\n",
    "                    #we keep track of the unknown words to see how well our model fits the data\n",
    "                    unknown_words.append(word)\n",
    "    # Dividing the result by number of words to get average\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating the average feature vector\n",
    "def getAvgFeatureVecs(texts, stopwords, model, modelword_index, num_features):\n",
    "    counter = 0\n",
    "    textFeatureVecs = np.zeros((len(texts),num_features),dtype=\"float32\")\n",
    "    for text in texts:\n",
    "        # Printing a status message every 1000th text\n",
    "        if counter%200 == 0:\n",
    "            print(\"Review %d of %d\"%(counter,len(texts)))\n",
    "            \n",
    "        textFeatureVecs[counter] = featureVecMethod(text, stopwords, model, modelword_index,num_features)\n",
    "        counter = counter+1\n",
    "    return textFeatureVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to load the same word embedding model as we used before to get compatible word embeddings for this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader as api\n",
    "# download the model and return as object ready for use\n",
    "word_embedding_model = api.load(\"glove-twitter-25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piek/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 3613\n",
      "Review 200 of 3613\n",
      "Review 400 of 3613\n",
      "Review 600 of 3613\n",
      "Review 800 of 3613\n",
      "Review 1000 of 3613\n",
      "Review 1200 of 3613\n",
      "Review 1400 of 3613\n",
      "Review 1600 of 3613\n",
      "Review 1800 of 3613\n",
      "Review 2000 of 3613\n",
      "Review 2200 of 3613\n",
      "Review 2400 of 3613\n",
      "Review 2600 of 3613\n",
      "Review 2800 of 3613\n",
      "Review 3000 of 3613\n",
      "Review 3200 of 3613\n",
      "Review 3400 of 3613\n",
      "Review 3600 of 3613\n"
     ]
    }
   ],
   "source": [
    "index2word_set = set(word_embedding_model.wv.index2word)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "num_features = 25\n",
    "tweet_tokens = []\n",
    "for tweet in tweet_instances:\n",
    "    tweet_tokens.append(nltk.tokenize.word_tokenize(tweet))\n",
    "\n",
    "tweet_embedding_vectors = getAvgFeatureVecs(tweet_tokens, stop_words, word_embedding_model, index2word_set, num_features)\n",
    "#### Due to the averaging, there could be infinitive values or NaN values. The next numpy function turns these value to \"0\" scores\n",
    "tweet_embedding_vectors = np.nan_to_num(tweet_embedding_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to convert the labels to the same numeric valus that we used before. We should use our LabelEncoder that we used before. However, we want the labels to have the same numeric values as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "# first we instantiate a label encode\n",
    "le = preprocessing.LabelEncoder()\n",
    "# we fee this encoder with the complete list of labels from our data\n",
    "labels = ['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']\n",
    "le.fit(labels)\n",
    "print(list(le.classes_))\n",
    "tweet_classes = le.transform(tweet_labels)\n",
    "print(list(tweet_classes[0:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the first set of tweets are all expressing *anger* which makes sense given the way the data were concatenated from the separate files with tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Loading the classifier for embedding representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import gensim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename_classifier = 'svm_nonlinear_clf_embeddings.sav'\n",
    "\n",
    "# load the classifier and the vectorizer from disk\n",
    "svm_nonlinear_clf = pickle.load(open(filename_classifier, 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have classifier make a prediction\n",
    "tweet_pred_nonlinear_embeddings = svm_nonlinear_clf.predict(tweet_embedding_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger' 'disgust' 'fear' 'joy' 'neutral' 'sadness' 'surprise']\n",
      "SVM NONLINEAR EMBEDDINGS ----------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0  0.0000000 0.0000000 0.0000000       857\n",
      "           2  0.0000000 0.0000000 0.0000000      1147\n",
      "           3  0.3990536 0.3074119 0.3472889       823\n",
      "           4  0.0000000 0.0000000 0.0000000         0\n",
      "           5  0.0000000 0.0000000 0.0000000       786\n",
      "           6  0.0000000 0.0000000 0.0000000         0\n",
      "\n",
      "    accuracy                      0.0700249      3613\n",
      "   macro avg  0.0665089 0.0512353 0.0578815      3613\n",
      "weighted avg  0.0908998 0.0700249 0.0791084      3613\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piek/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/piek/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(tweet_classes,tweet_pred_nonlinear_embeddings,digits = 7)\n",
    "print(le.classes_)\n",
    "print('SVM NONLINEAR EMBEDDINGS ----------------------------------------------------------------')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Representing and classifying the tweets as a bag-of-words vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Representing the tweets as BoW vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to represent the tweets according to our BoW vector representation derived from the MELD data, we need to load the *utterance_vec* file that we saved before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the classifier and the vectorizer from disk\n",
    "filename_vectorizer = 'utterance_vec.sav'\n",
    "loaded_vectorizer = pickle.load(open(filename_vectorizer, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now apply the transform function to the tokenized tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3613, 1151)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#tfidf_transformer = TfidfTransformer()\n",
    "#utterance_tfidf = tfidf_transformer.fit_transform(utterance_counts)\n",
    "tweet_vectors = loaded_vectorizer.transform(tweet_instances)\n",
    "tweet_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the matrix has vectors of the same size as before for the BoW token vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now load the classifier and make the prediction in this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the classifier on disk\n",
    "filename_classifier = 'svm_linear_clf_bow.sav'\n",
    "\n",
    "loaded_bow_classifier = pickle.load(open(filename_classifier, 'rb'))\n",
    "\n",
    "pred_from_loaded_bow_classifier = loaded_bow_classifier.predict(tweet_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger' 'disgust' 'fear' 'joy' 'neutral' 'sadness' 'surprise']\n",
      "SVM LINEAR BOW ----------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0  0.2413793 0.0816803 0.1220575       857\n",
      "           1  0.0000000 0.0000000 0.0000000         0\n",
      "           2  0.5714286 0.0069747 0.0137812      1147\n",
      "           3  0.3736655 0.3827461 0.3781513       823\n",
      "           4  0.0000000 0.0000000 0.0000000         0\n",
      "           5  0.3781095 0.0966921 0.1540020       786\n",
      "           6  0.0000000 0.0000000 0.0000000         0\n",
      "\n",
      "    accuracy                      0.1298090      3613\n",
      "   macro avg  0.2235118 0.0811562 0.0954274      3613\n",
      "weighted avg  0.4060369 0.1298090 0.1529683      3613\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(tweet_classes,pred_from_loaded_bow_classifier,digits = 7)\n",
    "print(le.classes_)\n",
    "print('SVM LINEAR BOW ----------------------------------------------------------------')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
